{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Things covered in this module:\n","\n","\n","\n","1.Novelty Detection \n","\n","2.Validity Resoning \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install accelerate protobuf sentencepiece torch git+https://github.com/huggingface/transformers huggingface_hub\n","!pip install llama-index-llms-huggingface\n","!pip install -U langchain-community\n","!pip install llama-index-embeddings-langchain\n","!pip install sentence-transformers\n","\n","!pip install llama-index"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:18:27.548482Z","iopub.status.busy":"2024-08-13T09:18:27.548134Z","iopub.status.idle":"2024-08-13T09:18:27.554886Z","shell.execute_reply":"2024-08-13T09:18:27.554131Z","shell.execute_reply.started":"2024-08-13T09:18:27.548453Z"},"trusted":true},"outputs":[],"source":["import os \n","os.environ['TOKENIZERS_PARALLELISM'] = \"True\""]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:18:27.955257Z","iopub.status.busy":"2024-08-13T09:18:27.954882Z","iopub.status.idle":"2024-08-13T09:18:32.497184Z","shell.execute_reply":"2024-08-13T09:18:32.496396Z","shell.execute_reply.started":"2024-08-13T09:18:27.955225Z"},"trusted":true},"outputs":[],"source":["from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext\n","from llama_index.llms.huggingface import HuggingFaceLLM\n","from llama_index.core.prompts.prompts import SimpleInputPrompt\n","import torch\n","from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n","from llama_index.core import ServiceContext\n","from llama_index.embeddings.langchain import LangchainEmbedding"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:18:32.499015Z","iopub.status.busy":"2024-08-13T09:18:32.498566Z","iopub.status.idle":"2024-08-13T09:18:37.257423Z","shell.execute_reply":"2024-08-13T09:18:37.256556Z","shell.execute_reply.started":"2024-08-13T09:18:32.498991Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[Document(id_='04bb4af2-fc53-445a-8cdc-160500f53414', embedding=None, metadata={'page_label': '1', 'file_name': 'a survey of ai t2i and t2v.pdf', 'file_path': '/kaggle/input/demo-db/a survey of ai t2i and t2v.pdf', 'file_type': 'application/pdf', 'file_size': 403891, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of AI Text-to-Image and AI Text-to-Video Generators Aditi Singh  Department of Computer Science Kent State University Ohio, USA asingh37@kent.edu Abstract— Text-to-Image and Text-to-Video AI generation models are revolutionary technologies that use deep learning and natural language processing (NLP) techniques to create images and videos from textual descriptions. This paper investigates cutting-edge approaches in the discipline of Text-to-Image and Text-to-Video AI generations. The survey provides an overview of the existing literature as well as an analysis of the approaches used in various studies. It covers data preprocessing techniques, neural network types, and evaluation metrics used in the field. In addition, the paper discusses the challenges and limitations of Text-to-Image and Text-to-Video AI generations, as well as future research directions. Overall, these models have promising potential for a wide range of applications such as video production, content creation, and digital marketing. Keywords — artificial intelligence; deep learning; natural language processing (NLP); large language model; AI text-to-image generation; AI text-to-video generation; DALL-E; CogView; Imagen; NUWA; Phenaki; GODIVA. I. INTRODUCTION With rapid advancements of deep learning and natural language processing (NLP) techniques in recent years, AI text-to-image and AI text-to-video generators have emerged as an advanced powerful tool that allows generation of images and videos from textual descriptions. These AI generators analyze the textual data using advance and complex techniques such as attention based Recurrent Neural Network [1], Generative Adversarial Network [2], and transformers [3][4] in order to generate corresponding high-quality images or videos.  The motivation after the AI text-to-image and AI text-to-video generators is driven by the necessity to automate the content creation process, making it faster for producing diverse content in proficient and economical way. These systems have potential applications in different fields such as marketing, education, and entertainment content creation. For instance, in marketing AI text-to-image generators can create product design, catalogs and user manuals [5]. In education, AI text-to-video generators can be used for creating instructional videos and animation to improve overall learning experience [6]. In entertainment industry, AI text-to-image generator and AI text-to-video generator can be used to create movie promotional videos, teasers, and more [7]. Overall, these generators aim to enhance user engagement and improve the user experience. However, there are several limitations and challenges with the rapid development of AI text-to-image and AI text-to-video generators. The major challenge is the need of a larger high-quality training dataset. It can be challenging to obtain and label a large dataset for training. Another challenge is the lack of interpretability of the generated outputs which makes it difficult to understand the reasoning behind the generated visual content. Besides, these systems may not always be associated with the intended message or vision, leading to errors and conflicts in the generated output. Another challenge is the trade-off between visual quality and processing time. Producing high-quality images and videos can be computationally costly and slow, making it difficult to generate large amounts of content fast. Also, the produced content may not always be associated with social or public norms, leading to misinterpretation or misrepresentation of the envisioned message. These limitations and drawbacks must be carefully studied when utilizing AI text-to-image and text-to-video generators in practice. This paper aims to provide an overview of the current state-of-the-art techniques in both AI text-to-image and text-to-video generators. Primarily to examine the underlying technologies such as data preprocessing techniques, the types of neural networks, and the evaluation metrics used in the AI text-to-image and text-to-video generators.  The paper is structured as follows. Section II provides an overview of AI text-to-image generators, including popular techniques and comparison of their capabilities. Section III explores popular AI text-to-videos generators and provide a comparison of their capabilities. Section IV present the analysis of the current state-of-the-art in both AI text-to-image and AI text-to-videos generators. Finally Section V draws conclusion. II. AI TEXT-TO-IMAGE GENERATOR AI text-to-image generator are powerful tools that are equipped with natural language processing capabilities and computer vision to generate images.  Let’s discuss some of the popular state-of-the-art text-to-image generators such as  CogView2, DALL-E 2 and Imagen. Table 1 provides details of these systems. A. CogView2 CogView2 [8] is an AI text-to-image generator that uses a hierarchical transformer-based approach to generate images from textual descriptions. Cogview2 uses the Cross-Modal general Language Model (CogLM), a pre-trained 6B-parameter transformer with a self-supervised task to mask and predict various types of tokens in a text and image token sequence. The hierarchical design of CogView2 allows for fast and efficient generation of high-resolution images by generating low-resolution images first and then refining them via an iterative super-resolution module that uses local parallel autoregressive generation. CogView2 is 10 times faster than CogView [9], which used sliding-window super-resolution, for generating images of similar resolution and better quality. Additionally, CogView2 supports interactive text-guided editing of images.    ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='ad253f5b-efcc-4eab-923a-35143e0bb2c9', embedding=None, metadata={'page_label': '2', 'file_name': 'a survey of ai t2i and t2v.pdf', 'file_path': '/kaggle/input/demo-db/a survey of ai t2i and t2v.pdf', 'file_type': 'application/pdf', 'file_size': 403891, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE I.  AI TEXT-TO-IMAGE GENERATORS AND THEIR COMPARISON   Architecture Training Data Image Quality Computational Requirements Interpretability Novelty CogView2 Hierarchical transformer-based Large-scale text-only datasets High-resolution, better quality Relatively efficient Difficult to interpret Fast and efficient generation DALL-E 2 Large-scale transformer language model with StyleGAN2 architecture Large dataset of image-text pairs High-quality and diverse High computational cost Easy to interpret Complex and diverse textual prompts Imagen Large-scale frozen T5-XXL encoder and diffusion models Large text-only corpora High-quality and photorealistic Relatively efficient Easy to interpret Leveraging existing language models for image generation B. Dall-E 2 DALL-E 2 [10] is another state-of-the-art AI text-to-image generator. It is built by OpenAI upon the success of the original DALL-E model.  The main idea behind DALL-E 2 is to generate high-resolution (1024x1024) images from textual input by training a large transformer model with a 175B parameter , making it the largest language model trained to date. Unlike the original DALL-E, which used a simple VQ-VAE architecture for image generation, DALL-E 2 uses a StyleGAN2 architecture, which is a more powerful generative model that can generate more realistic and diverse images. Additionally, DALL-E 2 can handle more complex and diverse textual prompts, such as questions or instructions, and can generate a wider range of objects and scenes. To train DALL-E 2, OpenAI collected a large dataset of image-text pairs and used a multi-stage training process that combines a pretraining stage on a large text corpus with a finetuning stage on the image-text dataset. During inference, given a textual prompt, DALL-E 2 generates a sequence of image tokens in an autoregressive manner, with each token representing a patch of the final image. Finally, these image tokens are passed through the StyleGAN2 generator to produce the final high-resolution image. DALL-E 2 has shown impressive results in generating high-quality and diverse images that are closely related to the input text. However, it still suffers from the autoregressive generation process that limits its speed and scalability, and the high computational cost of training a large-scale transformer model on a large image dataset. C. Imagen Google Imagen [11] is AI text-to-image generator that combines the power of large transformer language models and the strength of diffusion models to generate high-quality image. It is built on a large frozen T5-XXL encoder, which encodes the input text into embeddings, and a conditional diffusion model that maps the text embedding into a 64x64 image. In addition, Imagen utilizes text-conditional super-resolution diffusion models to upsample the image from 64x64 to 256x256 and from 256x256 to 1024x1024. Imagen achieves state-of-the-art results in terms of FID score and image-text alignment on the COCO dataset and outperforms recent methods in side-by-side comparisons on the comprehensive and challenging benchmark for text-to-image models, DrawBench. Imagen discovered that large-scale language models, such as T5, can be highly efficient in encoding text for the purpose of image synthesis. This is especially true when the models have been pre-trained on text-only corpora, indicating the possibility of leveraging existing language models for image-generation tasks. This allows for a higher degree of photorealism and a deeper level of language understanding in the generated images. As seen in above discussion, these popular AI text-to-image generators like CogView2, DALL-E 2, and Imagen use a variety of methods to produce images from text input. The hierarchical transformer-based method used by CogView2 enables the quick and effective creation of high-resolution images. DALL-E 2 is based on a substantial transformer language model and employs a potent StyleGAN2 architecture to produce a wide range of lifelike visuals. Imagen blends the strength of diffusion models with the power of massive transformer language models to create high-quality images. The ability of all three to produce various, high-quality images that are closely related to the input text has produced outstanding results. III. AI TEXT-TO-VIDEO GENERATOR AI text-to-video generators have drawn a lot of interest recently because of their potential to completely change the video production sector. With the help of these generator, users may quickly and easily create highly personalized and interesting video material. These systems make use of recent developments in deep learning and natural language processing to generate films from written descriptions. While the quality and variety of videos that early AI text-to-video generators could make were constrained, newer improvements have demonstrated encouraging results in producing a wide range of extremely realistic videos. The capacity to generate videos with high levels of consistency and the requirement for large computational resources are just two of the limitations.  The following section will discuss state-of-the-art AI text-to-video generators such as Make-A-Video, Imagen Video, Phenaki, GODIVA and CogVideo highlighting their advantages, disadvantages, and prospective uses.  Table 2 provides details of these models. A. Make-A-Video  Make-A-Video [12] is an innovative method that extends a diffusion-based text-to-image (T2I) model to text-to-video (T2V) generation through a spatiotemporally factorized diffusion model. By leveraging joint text-image priors, this approach eliminates the need for paired text-video data, enabling the potential scaling to larger amounts of video data.  For the first time, super-resolution strategies in both spatial and temporal dimensions are presented, generating high-definition, high frame-rate videos based on user ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='7cdbfff6-6146-4c96-b5d4-105d2d7374a9', embedding=None, metadata={'page_label': '3', 'file_name': 'a survey of ai t2i and t2v.pdf', 'file_path': '/kaggle/input/demo-db/a survey of ai t2i and t2v.pdf', 'file_type': 'application/pdf', 'file_size': 403891, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='provided textual input. Make-A-Video is thoroughly evaluated against existing T2V systems, showcasing state-of-the-art results in both quantitative and qualitative measures. This evaluation surpasses the existing literature in the T2V domain.  B. Imagen Video Imagen Video [13] utilizes a frozen T5 text encoder, a base video diffusion model, and interleaved spatial and temporal super-resolution diffusion models to generate high quality videos. The system has been scaled up to generate 128 frame 1280x768 high-definition videos at 24 frames per second. Additionally, the system has a high degree of controllability and world knowledge, which allows it to generate diverse videos and text animations in various artistic styles, as well as with 3D object understanding. The design decisions made in the system, such as the use of fully-convolutional temporal and spatial super-resolution models and the v-parameterization of diffusion models, contribute to its successful performance C. Phenaki  Phenaki [14] is another efficient and lightweight model from Google  that can generate videos from short text inputs. However, it is limited to simple actions and movements and lacks fine-grained details. Phenaki is a text-to-video model that can generate long, temporally coherent and diverse videos conditioned on open-domain prompts and even sequences of prompts that tell a story.  To achieve this, Phenaki introduces a novel encoder-decoder architecture called C-ViViT, which compresses videos to discrete embeddings (tokens) and exploits temporal redundancy to improve reconstruction quality while compressing the number of video tokens. The model also uses a transformer to translate text embeddings generated by a pre-trained language model, T5X, to video tokens. Phenaki is trained on both text-to-video and text-to-image datasets and demonstrates the ability to generalize beyond what is available in the video datasets.   D. CogVideo CogVideo [15] is a large-scale pretrained text-to-video generative model, which is of 9.4 billion parameters and trained on 5.4 million text-video pairs. This model utilizes the foundation provided by the pretrained text-to-image model, CogView2, effectively harnessing the knowledge acquired during the text-image pretraining phase. The model is designed to generate high-resolution (480x480) videos from natural language descriptions. To ensure the alignment between text and its temporal counterparts in the video, CogVideo uses a multi-frame-rate hierarchical training strategy. This allows the model to control the intensity of changes during the generation and significantly improves the generation accuracy, especially for movements of complex semantics.  E. GODIVA GODIVA [16] is a cutting-edge text-to-video generation model that employs a Transformer architecture, pretrained on an extensive text corpus. It is capable of producing high-quality videos with increased model capacity, albeit at the expense of computational resources and extensive training data requirements. The model comprises a VQ-VAE autoencoder trained to represent continuous video pixels as discrete video tokens, and a 3D sparse attention model trained using language input and discrete video tokens as labels. This attention mechanism considers temporal, column, and row information to generate videos effectively.  GODIVA is pretrained on the HowTo100M dataset, and it demonstrates impressive video generation performance in both fine-tuning and zero-shot settings. F. NUWA NUWA [17] is a unified multimodal pre-trained model designed for visual synthesis tasks, including image and video generation and manipulation. It is a 3D transformer encoder-decoder framework that covers language, image, and video for different visual synthesis scenarios. The encoder takes text or visual sketch as input, and the decoder is shared by eight visual synthesis tasks. To reduce computational complexity and improve the visual quality of the generated results, NUWA employs a 3D Nearby Attention (3DNA) mechanism that considers the locality characteristic of both spatial and temporal axes. 3DNA allows NUWA to efficiently process high-dimensional visual data, making it possible to scale up to larger and more complex visual synthesis tasks. NUWA has been evaluated on eight downstream visual synthesis tasks and compared with several strong baselines, achieving state-of-the-art results in text-to-image generation, text-to-video generation, video prediction, and more. It also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks, meaning that it can perform these tasks without any explicit training data. IV. Analysis The current state-of-the-art in both AI text-to-image and text-to-video generators have demonstrated great breakthroughs. However, there exist some challenges that need to be addressed.  AI text-to-image generators have produced high-quality images with increased diversity and photorealism, and some models even offer interactive text-guided image manipulation. Unfortunately, these generators continue to necessitate a large amount of computational resources, limiting their scalability and accessibility. Moreover, existing models have depended heavily on pre-existing datasets, limiting their applicability to specific fields. Future study should emphasize improving the efficiency and availability of these generators, as well as their applicability to other areas.  AI text-to-video generators have demonstrated excellent successes in creating very realistic and customized videos, but they still face issues in generating videos with high degrees of coherence and demanding large processing resources. Recent developments generate high-quality videos using approaches such as large-scale pre-trained language models and generative models such as GANs and diffusion models, however these models are computationally costly and have scaling constraints. Future research should focus on creating new ways to enhance the production process and minimize processing expenses to make text-to-video generators more accessible and efficient.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='98c897e8-c02b-460b-9591-d805c271627e', embedding=None, metadata={'page_label': '4', 'file_name': 'a survey of ai t2i and t2v.pdf', 'file_path': '/kaggle/input/demo-db/a survey of ai t2i and t2v.pdf', 'file_type': 'application/pdf', 'file_size': 403891, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE II.  AI TEXT -TO-VIDEO GENERATOR AND THEIR COMPARISON Video Generator Model Type Advantages Limitations Make-A-Video 2I (Text-to-Image) models and unsupervised learning on unlabeled video data Accelerated training, Unsupervised learning, Inheritance of image generation models. Cannot learn associations between text and certain phenomena in videos. Imagen Video cascade of video diffusion models High fidelity, Diverse video generation, 3D object understanding, Text animations Trained on problematic data [18][19][20], social biases, and stereotypes. Phenaki Encoder-decoder model with a transformer Good performance on video prediction, can generate long videos conditioned on text and starting frame Trained on biased datasets. GODIVA Text-to-video pretrained model with three-dimensional sparse attention mechanism Reduced computation cost, Good zero-shot capability Challenge to generate long videos with high resolution, evaluating text-to-video generation task remains a challenge. CogVideo Inherits pretrained text-to-image model CogView2 Efficiently leverages image generation capacity, better understanding of text-video relations Restriction on input sequence length, large scale model and limitation of GPU memory. NUWA  Multimodal pretrained model with 3D transformer encoder-decoder framework Reduces computational complexity, Good zero-shot capabilities Poor text-video alignment in frames. V. CONCLUSION In this paper, a brief description of different types of AI text-to-image and AI text-to-video generators has been presented. The future of AI text-to-image and AI text-to-video generators appears bright. Continued research and development in these areas are likely to result in more efficient, powerful, and accessible systems that can revolutionize the way users produce and interact with digital content. AI text-to-image and AI text-to-video generators have the potential to usher in a new era of creativity and productivity in a variety of industries due to their ability to create high-quality images and videos from textual descriptions. As such, they will undoubtedly stay an active area of research and development in the coming years. REFERENCES  [1] T. Zia, S. Arif, S. Murtaza, and M. A. Ullah, \"Text-to-Image Generation with Attention Based Recurrent Neural Networks,\" arXiv preprint arXiv:2001.06658, 2020.  [2] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, \"Generative adversarial text to image synthesis,\" in Proceedings of the 33rd International Conference on Machine Learning, New York, NY, USA, 2016, pp. 1060-1069  [3] N. A. Fotedar and J. H. Wang, \"Bumblebee: Text-to-Image Generation with Transformers,\" in Proceedings of the 2019 IEEE International Conference on Image Processing (ICIP), Taipei, Taiwan, 2019, pp. 3465-3469.  [4] H. Chang, H. Zhang, J. Barber, A. J. Maschinot, J. Lezama, L. Jiang, M. -H. Yang, K. Murphy, W. T. Freeman, M. Rubinstein, Y. Li, and D. Krishnan, \"Muse: Text-To-Image Generation via Masked Generative Transformers,\" arXiv preprint arXiv:2301.00704, 2023.  [5] A. Haleem, M. Javaid, M. A. Qadri, R. P. Singh, and R. Suman, \"Artificial intelligence (AI) applications for marketing: A literature-based study,\" International Journal of Intelligent Networks, vol. 3, pp. 119-132, 2022. doi: 10.1016/j.ijin.2022.08.005.  [6] S. Aktay, \"The usability of Images Generated by Artificial Intelligence (AI) in Education,\" International Technology and Education Journal, vol. 6, no. 2, pp. 51-62, 2022. [Online]. Available: https://dergipark.org.tr/en/pub/itej/issue/75198/1233537.  [7] E. Cetinic and J. She, \"Understanding and Creating Art with AI: Review and Outlook,\" ACM Trans. Multimedia Comput. Commun. Appl., vol. 18, no. 2, Article 66, May 2022, pp. 1-22, doi: 10.1145/3475799.  [8] M. Ding, W. Zheng, W. Hong, and J. Tang, \"CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers,\" arXiv, 2022. [Online]. Available: https://arxiv.org/abs/2204.14217. [Accessed: March 18, 2023]. [9] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou, Z. Shao, H. Yang, and J. Tang, \"CogView: Mastering Text-to-Image Generation via Transformers,\" arXiv:2105.13290 [cs.CV], 2021.  [10] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \"Hierarchical Text-Conditional Image Generation with CLIP Latents,\" in arXiv preprint arXiv:2202.10775, 202  [11] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. Seyed Ghasemipour, B. Karagol Ayan, S. S. Mahdavi, R. G. Lopes, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi, \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,\" arXiv:2205.11487 [cs.CV], May 2022. [12] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, D. Parikh, S. Gupta, and Y. Taigman, \"Make-A-Video: Text-to-Video Generation without Text-Video Data,\" arXiv:2209.14792 [cs.CV], Sep. 2022. [13] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, and T. Salimans, \"Imagen Video: High Definition Video Generation with Diffusion Models,\" arXiv preprint arXiv:2210.02303, Oct. 2022. [Online]. Available: https://arxiv.org/abs/2210.02303. [14] R. Villegas, M. Babaeizadeh, P.-J. Kindermans, H. Moraldo, H. Zhang, M. T. Saffar, S. Castro, J. Kunze, and D. Erhan, \"Phenaki: Variable Length Video Generation from Open Domain Textual Description,\" arXiv:2210.02399 [cs.CV], Oct. 2022. [15] W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang, \"CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,\" arXiv:2205.15868 [cs.CV], May 2022. [16] C. Wu, L. Huang, Q. Zhang, B. Li, L. Ji, F. Yang, G. Sapiro, and N. Duan, \"GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions,\" arXiv:2104.14806, Apr. 2021. [17] C. Wu, J. Liang, L. Ji, F. Yang, Y. Fang, D. Jiang, and N. Duan, \"NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,\" arXiv:2111.12417 [cs.CV], Nov. 2021.  [18] B. Bordia and S. R. Bowman, \"Identifying and Reducing Gender Bias in Word-Level Language Models,\" arXiv:1904.03035 [cs.CL], 2019. [19] A. Birhane, V. U. Prabhu, and E. Kahembwe, \"Multimodal Datasets: Misogyny, Pornography, and Malignant Stereotypes,\" arXiv:2110.01963, 2021. [20] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, \"On the dangers of stochastic parrots: Can language models be too big?\" in Proc. FAccT, 2021. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='de971b99-a448-4e7a-a553-a274324cc810', embedding=None, metadata={'page_label': '1', 'file_name': 'design and implementation.pdf', 'file_path': '/kaggle/input/demo-db/design and implementation.pdf', 'file_type': 'application/pdf', 'file_size': 257923, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/313389686\\nText to Speech Conversion\\nArticle \\xa0\\xa0 in\\xa0\\xa0Indian Journal of Scienc e and T echnolog y · Oct ober 2016\\nDOI: 10.17485/ ijst/2016/v9i38/102967\\nCITATIONS\\n36READS\\n48,638\\n4 author s:\\nS. Venk atesw arlu\\nK L Univ ersity\\n28 PUBLICA TIONS \\xa0\\xa0\\xa094 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nDuvvuri B K Kamesh Duvvuri\\nMalla Malla R eddy Engineering c olle ge for Women\\n34 PUBLICA TIONS \\xa0\\xa0\\xa0177 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nSastr y Jammalamadak a\\nK L Univ ersity\\n95 PUBLICA TIONS \\xa0\\xa0\\xa0493 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nChint ala R adhik a Rani\\nK L Univ ersity\\n22 PUBLICA TIONS \\xa0\\xa0\\xa077 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Duvvuri B K Kamesh Duvvuri  on 23 Sept ember 2017.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='f89385c1-66fd-4fee-9f50-59eafb5b001a', embedding=None, metadata={'page_label': '2', 'file_name': 'design and implementation.pdf', 'file_path': '/kaggle/input/demo-db/design and implementation.pdf', 'file_type': 'application/pdf', 'file_size': 257923, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='*Author for correspondenceIndian Journal of Science and Technology,  Vol 9(38), DOI: 10.17485/ijst/2016/v9i38/102967, October 2016ISSN (Print) : 0974-6846 \\nISSN (Online) : 0974-5645\\nText to Speech Conversion\\nS. Venkateswarlu1*, D. B. K. Kamesh1, J. K. R. Sastry2 and Radhika Rani2\\n1Department of CSE , K L University, Vaddeswarm, Guntur – 522502, Andhra Pradesh, India; somu23@kluniversity.in, \\n                kameshdbk@kluniversity.in, radhikarani_cse@kluniversity.in\\n2Department of ECM, K L University, Vaddeswarm, Guntur – 522502, Andhra Pradesh, India; \\ndrsastry@kluniversity.in\\nKeywords:  Image Processing, OCR, Text Extraction, Text-to-speech, Voice ProcessingAbstract\\nThe present paper has introduced an innovative, efficient and real-time cost beneficial technique that enables user to hear \\nthe contents of text images instead of reading through them. It combines the concept of Optical Character Recognition \\n(OCR) and Text to Speech Synthesizer (TTS) in Raspberry pi. This kind of system helps visually impaired people to interact \\nwith computers effectively through vocal interface. Text Extraction from color images is a challenging task in computer \\nvision. Text-to-Speech conversion is a method that scans and reads English alphabets and numbers that are in the image \\nusing OCR technique and changing it to voices. This paper describes the design, implementation and experimental results \\nof the device. This device consists of two modules, image processing module and voice processing module. The device was \\ndeveloped based on Raspberry Pi v2 with 900 MHz processor speed.\\n1. Introduction\\nOptical character Recognition (OCR) is a process that \\nconverts scanned or printed text images1, handwritten \\ntext into editable text for further processing. This paper \\nhas presented a robust approach for text extraction and \\nconverting it to speech. Testing of device was done on \\nraspberry pi platform. The Raspy is initially connected \\nto the internet through VLAN. The software is installed \\nusing command lines. Following steps are to be followed:\\n1. The first setup is to download the installation script, \\n2. Second step is to convert it to executable form and \\n3. The last step starts the script which does the rest of \\nthe installation work. \\nDevice set up is done as shown in Figure 1. The web-\\ncam is manually focused towards the text. Then, it takes \\na picture; a delay of around 7 seconds is provided, which \\nhelps to focus the webcam, if it is accidently defocused. \\nAfter delay, picture is taken and processed by Raspy to \\nhear the spoken words of the text through the earphone \\nor speaker plugged into Raspy through its audio jack. \\nFigure 1. Block diagram of text to speech conversion.\\n2. Methodology\\nText-to-speech device consists of two main modules, the \\nimage processing module and voice processing modules. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='bb27160c-3711-4d68-8172-d74c0190e2cb', embedding=None, metadata={'page_label': '3', 'file_name': 'design and implementation.pdf', 'file_path': '/kaggle/input/demo-db/design and implementation.pdf', 'file_type': 'application/pdf', 'file_size': 257923, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Indian Journal of Science and Technology Vol 9 (38) | October 2016 | www.indjst.org 2\\nText to Speech ConversionImage processing module captures image using camera, \\nconverting the image into text. Voice processing mod-ule changes the text into sound and processes it with specific physical characteristics so that the sound can be understood. Figure 2 shows the block diagram of Text-To-Speech device, 1\\nst block is image processing module, \\nwhere OCR converts .jpg to .txt form. 2nd is voice process-\\ning module which converts .txt to speech \\nFigure 2. Block diagram of text-to-speech device. \\nFigure 2shows the block diagram of Text-To-Speech \\ndevice, 1st block is image processing module, where OCR \\nconverts .jpg to .txt form. 2nd is voice processing module \\nwhich converts .txt to speech. OCR is important element in this module. OCR or Optical Character Recognition is a technology that automatically recognize the character through the optical mechanism, this technology imitate the ability of the human senses of sight, where the cam-era becomes a replacement for eye and image processing is done in the computer engine as a substitute for the human brain\\n2. Tesseract OCR is a type of OCR engine \\nwith matrix matching3. The selection of Tesseract engine \\nis because of its flexibility and extensibility of machines and the fact that many communities are active researchers to develop this OCR engine and also because Tesseract OCR can support 149 languages. In this project we are identifying English alphabets. Before feeding the image to the OCR, it is converted to a binary image to increase the recognition accuracy. Image binary conversion is done by using Imagemagick software, which is another open source tool for image manipulation. The output of OCR is the text, which is stored in a file (speech.txt). Machines still have defects such as distortion at the edges and dim light effect, so it is still difficult for most OCR engines to get high accuracy text\\n4. It needs some supporting and \\ncondition in order to get the minimal defect. Tesseract OCR Implementation. \\n 2.1 Software Design \\nSoftware processes the input image and converted into text format. The software implementation is showed in Figure 3.\\nFigure 3. Software design of image processing module. \\n2.2 The Voice Processing Module \\nIn this module text is converted to speech. The out-put of OCR is the text, which is stored in a file (speech.txt). Here, Festival software is used to convert the text to speech. Festival is an open source Text To Speech (TTS) \\n7,8 \\nsystem, which is available in many languages. In this proj-ect, English TTS\\n 9–11system is used for reading the text. \\n3. Results\\nObserved outcome of project: \\n•\\tText is extracted from the image and converted to audio. \\n•\\tIt recognizes both capital as well as small letters. \\n•\\tIt recognizes numbers as well. \\n•\\tRange of reading distance was 38-42cm. \\n•\\tCharacter font size should be minimum 12pt. \\n•\\tMaximum tilt of the text line is 4-5 degree from the vertical. \\n4. Conclusion\\nText-to-Speech device can change the text image input into sound with a performance that is high enough and a readability tolerance of less than 2%, with the average time processing less than three minutes for A4 paper size. This portable device, does not require internet connec-tion, and can be used independently by people. Through ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='839f600d-d488-4164-b11f-e24025a062e4', embedding=None, metadata={'page_label': '4', 'file_name': 'design and implementation.pdf', 'file_path': '/kaggle/input/demo-db/design and implementation.pdf', 'file_type': 'application/pdf', 'file_size': 257923, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Indian Journal of Science and Technology3\\nVol 9 (38) | October 2016 | www.indjst.org S. Venkateswarlu, D. B. K. Kamesh, J. K. R. Sastry and Radhika Ranithis method, we can make editing process of books or web \\npages easier. \\n5. References\\n1. A rchana A, Shinde D. Text pre-processing and text seg-\\nmentation for OCR. International Journal of Computer \\nScience Engineering and Technology. 2012:810–12. \\n2.\\n M\\nithe R, Indalkar S, Divekar N. Optical character recog-\\nnition. International Journal of Recent Technology and Engineering. 2013 Mar; 2(1). \\n3.\\n S\\nmith R. An overview of the Tesseract OCR engine, USA: \\nGoogle Inc; 2007. \\n4.\\n S\\nhah H, Shah A. Optical character recognition of Gujarati \\nnumerical. International Conference on Signals, Systems and Automation. 2009; 49–53. \\n5.\\n M\\nonk S. Raspberry pi cook. \\n6.\\n T\\next localization and extraction in images using mathemat-\\nical\\n m\\norphology and OCR Techniques; 2013.\\n7.\\n V\\nanitha E, Kasarla PK, Kuamarswamy E. Implementation \\nof text- to-speech for real time embedded system using Raspberry Pi processor. International Journal and Magazine of Engineering Technology Management and Research. 2015 Jul:1995.\\n8.\\n  K\\numar GS, Krishna MNVLM. Low cost speech recognition \\nsystem running on Raspberry Pi to support Automation applications. International Journal of Engineering Trends and Technology. 2015; 21(5).\\n9.\\n B\\nhargava A, Nath KV , Sachdeva P , Samel M. Reading assis-\\ntant for visually Impaired. International Journal of current Engineering and Technology. 2015 Apr; 5(2). \\n10.\\n G\\nomes LCT, Nagle EJ, Chiquito JG. Text-to-speech conver -\\nsion system for Brazilian Portuguese using a formant-based synthesis technique. LPS-DECOM-FEEC-Unicamp.\\n11.\\n S\\nim Liew Fong, Abdelrahman Osman Elfaki, Md Gapar \\nbin Md Johar & Kevin Loo Tow Aik, Mobile Language Translator, 5th Malaysian Conference in Software Engineering (Misses); 2011.\\n12.\\n K\\namesh DBK, Nazma SK, Sastry JKR, Venkateswarlu S. \\nCamera based text to speech conversion, obstacle and cur -\\nrency detection for blind persons. Indian Journal of Science and Technology. 2016 Aug; 9(30).\\nView publication stats', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='4e2457a3-53ba-4d15-9128-77dd89374197', embedding=None, metadata={'page_label': '1', 'file_name': 'implementation-of-text-to-speech-conversion-IJERTV3IS030548.pdf', 'file_path': '/kaggle/input/demo-db/implementation-of-text-to-speech-conversion-IJERTV3IS030548.pdf', 'file_type': 'application/pdf', 'file_size': 829055, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\n  \\n Implementation of Text to Speech Conversion  \\n \\nChaw Su Thu Thu1 , Theingi Zin 2 \\n1Department of Electronic Engineering, Mandalay Technological University, Mandal ay \\n2Department of Electronic Engineering, Mandalay Technological University, Mandal ay \\n \\n  \\nAbstract - Text -To-Speech (TTS) conversion is a computer -\\nbased system that can be able to read any text aloud, whether \\nit was directly introduced in the computer by an operator or \\nscanned and submitted to an Optical Character Recognition \\n(OCR ) system. While in text to speech, there are many \\nsystems which convert normal language text in to speech. The \\nmain aims of this paper are to  study  on  Optical  Character  \\nRecognition  with  speech synthesis  technology  and  to \\ndevelop  a  cost  effecti ve user  friendly  image  to  speech \\nconversion system using MATLAB.  In this work, the OCR \\nsystem is implemented for the recognition of capital English \\ncharacter A to Z and number 0 to 9. Each character is \\nrecognized at once. The recognized character is s aved as text \\nin notepad file. In this work a text -to-speech conversion \\nsystem that can get the text through image and directly input \\nin the computer then speech through that text using \\nMATLAB.  \\n \\n1. INTRODUCTION  \\n \\nSpeech synthesis is the artificial productio n of human \\nspeech. A computer system used for  this purpose  is  called  \\na  speech  synthesizer,  and  can be  implemented  in  \\nsoftware or hardware [2]. A text -to-speech (TTS) system \\nconverts normal language text into speech; other systems \\nrender symbolic  linguistic representations like phonetic \\ntranscriptions into speech.  \\nText-to-speech (TTS) convention transforms linguistic \\ninformation stored as data or text into speech. It is widely \\nused in audio reading devices for blind people now  a days \\n[6]. In the l ast few years however, the use of text -to-speech \\nconversion technology has grown far beyond the disabled \\ncommunity to become a major adjunct to the rapidly \\ngrowing use of digital voice storage for voice mail and \\nvoice response systems. Also developmen ts in Speech \\nsynthesis technology for various languages have already \\ntaken place.  \\n \\nThe Speech Application Programming Interface  or SAPI  \\nis an API developed by Microsoft  to allow the use of \\nspeech recognition  and speech synthesis  within Windows  \\napplications.  \\n \\n2. PROPOSED ALGORITHM  \\n \\nIn this work, there are two main parts:  \\n\\uf0b7 Optical C haracter Recognition System for Paper \\nText \\n\\uf0b7 Text to Speech Conversion  \\n  \\n \\n2.1. Optical character recognition system  \\n \\nIn this part, there are three portions as described in the \\nfollow:  \\n\\uf0b7 Template file Creation  \\n\\uf0b7 Creating the Neural Network  \\n\\uf0b7 Character Recognition  \\n \\n2.1.1. Template file creation .  Letter A to Z and \\nnumber 0 to 9 images are collected. Each image is changed \\ninto 5 x 7 character representation in single vector by using \\nstep 1 to  5 as described in the character recognition section . \\nThese data are saved as data file for training in neural \\nnetwork.  \\n \\n2.1.2.  Creating the neural network . A feedforward \\nneural network is used to set up for pattern recognition  with \\n25 hidden neurons. After creating the network, the weights \\nand biases of the network are also initial ized to be ready for \\ntraining. The goal is assigned between 0.01 and to 0.05. \\nThe created Neural Network is trained by using data file \\nand target file. The neural network has to be trained by \\nadjusting weight and bias of network until the performance \\nreach es to goal.  \\n \\n911Vol. 3 Issue 3, March - 2014International Journal of Engineering Research & Technology (IJERT)\\nIJERTIJERTISSN: 2278-0181\\nwww.ijert.org IJERTV3IS030548', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='3665c900-e4b5-4869-ac78-18c01e9d4c5d', embedding=None, metadata={'page_label': '2', 'file_name': 'implementation-of-text-to-speech-conversion-IJERTV3IS030548.pdf', 'file_path': '/kaggle/input/demo-db/implementation-of-text-to-speech-conversion-IJERTV3IS030548.pdf', 'file_type': 'application/pdf', 'file_size': 829055, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\n  \\n 2.1.3.  Character recognition . Figure 1 shows the \\nflowchart of OCR system.  \\nImage Acquiring and Reading\\nGray to Binary Image\\nSementation\\nFeature Extraction\\nClassificationTemplates  \\ntrained in NN \\nConvert E-Text\\nOpen text.txt as \\nfile for write\\nWrite in the text fileStart\\nRGB to Gay Image \\nEnd\\n \\n \\n \\nFigure  1. Flowchart of OCR system  \\n \\nThe following steps are implemented for character \\nrecognition.  \\n\\uf0b7 Firstly acquire the character image and the image \\nwas read.  \\n\\uf0b7 Second step is preprocessing step. In this step \\nfirstly the image is converted into gray scale. Then  \\nthis  gray  image is  converted  into  black  and  \\nwhite  image (binary image).  Firstly the threshold \\nis counted in gray image then according to that \\nthreshold it is converted into black and white \\nimage.  \\n\\uf0b7 Find the boundary of the character image. Crop \\nthe image to the edge.  \\n\\uf0b7 Character is extracted and resized in this step. \\nLetters are resized according to templates size.  \\n\\uf0b7 The resized bin ary image is changed into 5 x 7 \\ncharacter representation in single vector.  \\n\\uf0b7 Load templates that it can be matched the letters \\nwith the templates.  \\n\\uf0b7 Open the text.txt as file for write.  \\n\\uf0b7 Write in the text file and concatenate the letters.  \\nFeature extraction and  classification are the heart of \\nOCR. The character image is mapped to a higher level by extracting special characteristics and patterns of the image \\nin the feature extraction phase.  \\nThe  classifier  is  then  trained  with  the  extracted  \\nfeatures  for  classification  task. The classification stage \\nidentifies each input character image by considering the \\ndetected features. As Classifiers, Template Matching and \\nNeural Networks are used.  \\n \\n2.2. Text to speech conversion  \\n  \\nThe character image is converted i nto text and then \\ntext into speech. The algorithm is followed.  \\n\\uf0b7 Firstly check the condition that if Win 32 SAPI is \\navailable in the computer or not. If it is not \\navailable then error will be generated and Win 32 \\nSAPI library should be loaded in the compute r. \\n\\uf0b7 Gets the voice object from Win 32 SAPI.   \\n\\uf0b7 Compares the input string with Win 32 SAPI \\nstring.   \\n\\uf0b7 Extracts voice by firstly select the voice which are \\navailable in library.  \\n\\uf0b7 Choose the pace of voice.  \\n\\uf0b7 Initializes the wave player for convert the text into \\nspeech.  \\n\\uf0b7 Finally get the speech for given image.  \\nText to speech conversion for the e -text input that directly \\ntyped in computer is also executed by the above steps.  \\n \\n3. SIMULATION RESULTS  \\n \\nIn this work, the OCR system is implemented for the \\nrecogniti on of ca pital English character A to Z and number \\n0 to 9. Each character is recognized at one time. The \\nrecognized character is saved as text with notepad file. \\nThere are two portions in program; in the first portion it \\ngives the text output according to input  im age ,  then  it  \\nconvert that  text  into  the  speech. In the second portion, \\nthe e -text is directly input in computer,  then it is converted \\ninto speech.  \\nFirstly the input image of time new romance, font size 12,  \\nbold type characters is taken and then it  is converted into \\ntext. As shown in Figure 2, character “A” is cropped from \\nthe image and features are extracted. After that it is \\nconverted to text, saved in notepad file and speech \\nsimultaneously. Similarly, the test results for character “T” \\nis also il lustrated in Figure 3. The recognized character can \\nbe displayed in the command widow and can be save in \\nnotepad file as shown in Figure 4.   \\n \\n \\n(a) \\n912Vol. 3 Issue 3, March - 2014International Journal of Engineering Research & Technology (IJERT)\\nIJERTIJERTISSN: 2278-0181\\nwww.ijert.org IJERTV3IS030548', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='faa7c718-1ad5-4de7-94f3-f338c176f0b5', embedding=None, metadata={'page_label': '3', 'file_name': 'implementation-of-text-to-speech-conversion-IJERTV3IS030548.pdf', 'file_path': '/kaggle/input/demo-db/implementation-of-text-to-speech-conversion-IJERTV3IS030548.pdf', 'file_type': 'application/pdf', 'file_size': 829055, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\n  \\n \\n \\n(b) \\nFigure 2. (a) Character “A” converted into text  (b) “A” sound wave      \\n  \\n \\n(a) \\n \\n(b) \\nFigure 3. (a) Character “ T” converted into text  (b) \\n“T” sound wave      \\n \\n \\n(a) \\n \\n(b) \\nFigure 4 . (a) Output text in command window (b) Saved text in notepad \\n(character “A” and “T” )  \\nThe mathematical numbers are also successfully \\nconverted into text and then speech which is s hown in \\nFigure 5.  \\n \\n(a) \\n \\n \\n(b) \\nFigure 5 . (a) Number “5 ” converted into text  (b) Number “5 ” sound wave     \\n  \\nAnother type  of  font  character  is taken and again  it  \\nis  converted  into  text  and  then  speech successfully as \\nshown in Figure 6 and 7.  \\n913Vol. 3 Issue 3, March - 2014International Journal of Engineering Research & Technology (IJERT)\\nIJERTIJERTISSN: 2278-0181\\nwww.ijert.org IJERTV3IS030548', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='b4926039-18c9-448e-9ed5-19c7aae60e68', embedding=None, metadata={'page_label': '4', 'file_name': 'implementation-of-text-to-speech-conversion-IJERTV3IS030548.pdf', 'file_path': '/kaggle/input/demo-db/implementation-of-text-to-speech-conversion-IJERTV3IS030548.pdf', 'file_type': 'application/pdf', 'file_size': 829055, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"  \\n  \\n \\n \\n(a) \\n \\n \\n(b) \\nFigure 6. (a) Character “M ” converted into text  (b) Character “M ” sound \\nwave     \\n \\n \\n(a) \\n \\n(b) \\nFigure 7. (a) Number “ 2” converted into text  (b) Number “ 2” sound wave     \\n As illustrated in Figure 8, t he e-text that directly input \\nin computer  by typin g from keyboard , then it is also \\nconverted into speech successfully.  \\n \\n \\n(a) \\n \\n(b) \\nFigure 8. (a) E -text Input (b)Sound Wave “Hello, How are you?”  \\n \\n \\n4. CONCLUSION  \\n \\nIn this work, image into text and then that text into \\nspeech is converted by MATLAB. E -text int o speech is \\nalso converted successfully. By  this  approach  text  from  \\na  word document, Web  page  or  e -Book  can be read and  \\ncan generate  synthesized  speech  through  a  computer's  \\nspeakers. For image to text conversion, firstly  image is \\nconverted  into gray image. Gray image is converted into \\nbinary image by thresholding and then it is converted into \\ntext by MATLAB. Microsoft Win 32 SAPI library has been \\nused  to  build  speech  enabled  applications,  which  \\nretrieve  the  voice  and  audio  outpu t information  \\navailable  for  computer. In this work, one character can be \\nconverted into text at once. As a further extension, OCR \\nsystem can be developed for converting words or sentences \\nimage  into text.  \\n \\n \\n \\n \\n \\n914Vol. 3 Issue 3, March - 2014International Journal of Engineering Research & Technology (IJERT)\\nIJERTIJERTISSN: 2278-0181\\nwww.ijert.org IJERTV3IS030548\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='81dd6107-dc87-4752-848f-dca96317658a', embedding=None, metadata={'page_label': '5', 'file_name': 'implementation-of-text-to-speech-conversion-IJERTV3IS030548.pdf', 'file_path': '/kaggle/input/demo-db/implementation-of-text-to-speech-conversion-IJERTV3IS030548.pdf', 'file_type': 'application/pdf', 'file_size': 829055, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\n  \\n  \\nREFERENCES  \\n \\n1. Ainsworth, W.,  \"A  system  for  converting  English  text  into  \\nspeech,\"  Audio  and Electroacoustics, IEEE Transactions on , \\nvol.21, no.3, pp. 288 -290, Jun 1973  \\n2. Fushikida, Katsunobu; Mitome, Yukio; Inoue, Yuji, \"A Text to \\nSpeech Synthesizer for the Personal Computer,\" Consumer \\nElectro nics, IEEE Transactions on , vol.CE -28, no.3, pp.250 -256, \\nAug. 1982  \\n3. Hertz, S.,  \"English  text  to  speech  conversion with  delta,\" \\nAcoustics, Speech,  and Signal Processing, IEEE International \\nConference on ICASSP \\'86. , vol.11, no., pp.2427 -2430, Apr 19 86 \\n4. Lynch, M.R.; Rayner, P.J., \"Optical character recognition using a \\nnew connectionist model,\"  Image Processing  and  its  Applications,  \\n1989.,  Third  International Conference on , vol., no., pp.63 -67, 18 -\\n20 Jul 1989  \\n5. S. Furui, “Speaker independent isola ted word recognition using \\ndynamic features of speech spectrum”, IEEE Transactions on \\nAcoustic, Speech, Signal Processing, Vol.34, issue 1, Feb 1986, pp. \\n52-59. \\n6. Leija, L.; Santiago, S.; Alvarado, C.,  \"A  system of  text  reading \\nand  translation  to voice   for  blind  persons  ,\"  Engineering  in \\nMedicine  and  Biology  Society,  1996. Bridging Disciplines for \\nBiomedicine. Proceedings of the 18th Annual International \\nConference of the IEEE , vol.1, no., pp.405 -406 vol.1, 31 Oct -3 Nov \\n1996  \\n7. Tanprasert,  C.;  Koanantakool,  T.,  \"Thai  OCR:  a  neural  network  \\napplication,\"TENCON  \\'96.  Proceedings.  1996  IEEE  TENCON.  \\nDigital  Signal  Processing Applications , vol.1, no., pp.90 -95 vol.1, \\n26-29 Nov 1996  \\n8. Breen, A.P.,  \"The  future  role of  text  to  speech  synthesis  in  \\nautomated  services,\" Advances  in  Interactive  Voice  Technologies  \\nfor  Telecommunication  Services (Digest No: 1997/147), IEE \\nColloquium on , vol., no., pp.6/1 -6/5, 12 Jun 1997  \\n \\n915Vol. 3 Issue 3, March - 2014International Journal of Engineering Research & Technology (IJERT)\\nIJERTIJERTISSN: 2278-0181\\nwww.ijert.org IJERTV3IS030548', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='c3ea4ed5-9f62-42cc-83c3-20f9d4385f55', embedding=None, metadata={'page_label': '1', 'file_name': 't2s_synthesis.pdf', 'file_path': '/kaggle/input/demo-db/t2s_synthesis.pdf', 'file_type': 'application/pdf', 'file_size': 1351269, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Text to Speech Synthesis \\n \\n \\nHarini S \\nAssistant Professor \\nDepartment of Informa Ɵon Science and  \\nEngineering \\nBMS College Of Engineering \\nBengaluru, India \\nharinis.ise@bmsce.ac.in  \\n \\n \\nAbstract - Text-to-Speech (TTS) synthesis is a technology that \\nconverts wri Ʃen text into spoken language, allowing for the \\ngeneraƟon of natural -sounding speech. This process involves \\nvarious stages, including text analysis, linguis Ɵc processing, \\nand waveform synthesis. TTS systems use algorithms and \\nmodels to transform input text into corresponding speech \\nsignals, mimicking the intona Ɵon, rhythm, and prosody of \\nhuman speech. Over the years, TTS synthesis has evolved, \\nincorpora Ɵng neural network -based approaches to enhance \\nthe naturalness and expressiveness of generated speech. \\nApplicaƟons of TTS span across accessibility tools, voice \\nassistants, naviga Ɵon systems, and entertainment, oﬀering \\na versaƟle and impac ƞul soluƟon for conver Ɵng wriƩen \\ncontent into audible form. Ongoing research con Ɵnues to \\nreﬁne TTS systems, striving to achieve even greater levels of \\nrealism and adaptability in genera Ɵng human -like speech. \\n \\nKeywords - Text-to-Speech (TTS), Speech Synthesis, Natural \\nLanguage Processing (NLP), Waveform Synthesis, \\nLinguistic Processing, Neural Networks, Synthetic Speech, \\nAccessibility, Human-like Speech, Speech Signal \\nProcessing, Phonemes, Prosodic Features, Audio Synthesis.  \\n \\nI. INTRODUCTION \\n \\nText-to-Speech (TTS) technology has revolutionized the \\nway we interact with computers and devices by enabling \\nthem to convert written text into spoken words. TTS systems \\nutilize advanced linguistic algorithms and artificial \\nintelligence techniques to generate high-quality, natural-\\nsounding speech that closely resembles human speech \\npatterns and intonations. the primary goal of TTS is to \\nprovide an inclusive and accessible means of \\ncommunication for individuals with visual impairments or \\nthose who prefer auditory information. By converting \\nwritten text into speech, TTS technology allows people to \\nlisten to text-based content such as books, articles, web \\npages, and messages, enhancing their overall digital \\nexperience and facilitating information consumption. \\n \\nTTS systems have come a long way in recent years, thanks \\nto advancements in deep learning, neural networks, and \\ndata-driven approaches. Modern TTS models are trained on \\nvast amounts of multilingual and multi-speaker data, \\nenabling them to produce speech in different languages and \\nmimic various voices with remarkable accuracy and \\nexpressiveness. moreover, TTS has found applications \\nbeyond accessibility, benefiting industries like \\nentertainment, education, customer service, and more. It   \\n \\nManoj G M \\nDepartment of Informa Ɵon Science and  \\nEngineering \\nBMS College Of Engineering, \\nBengaluru, India \\nmanojgm.is21@bmace.ac.in  \\n \\n \\n \\n \\nenables voice assistants, virtual characters, and automated \\nsystems to communicate with users, enhancing user \\nexperience and making human-machine interactions more \\nintuitive and engaging. as TTS technology continues to \\nevolve, we can expect further improvements in speech \\nquality, customization options, and efficiency. These \\nadvancements will contribute to creating more immersive \\nand inclusive experiences, where technology seamlessly \\nintegrates with our daily lives, offering an enhanced way to \\nconsume and interact with information. TTS systems often \\nprovide options for personalization and customization. \\nUsers can choose different voices, accents, and speech rates \\naccording to their preferences. This flexibility allows \\nindividuals to tailor the synthesized speech to their liking, \\nenhancing their overall experience and engagement with the \\ncontent. \\n \\nII. PROBLEM STATEMENT \\n \\nText-to-speech synthesis is like teaching computers to talk. \\nThe main problem is making the computer-generated speech \\nsound as natural as possible, like a human speaking. This \\ninvolves dealing with different languages, making the voice \\nexpress emotions, and responding quickly. It's also \\nimportant to let users choose how the computer voice sounds \\nand to make sure it works well for people with disabilities. \\nIn simpler terms, the challenge is to make the computer talk \\nin a way that sounds real and is easy for everyone to \\nunderstand. Handling different languages and accents, \\nexpressing emotions, working in real-time, letting users \\ncustomize the voice, dealing with lots of words, and being \\nuseful for people with disabilities. Solving these challenges \\ninvolves using smart computer programs and algorithms that \\nunderstand language and sound to create better and more \\nhuman-like speech from written text. \\n \\nIII.  LITERATURE SURVEY \\n \\n[1]  Text to Speech Conversion based on Emotion using \\nRecurrent Neural Network.  \\nThe paper introduces an enhanced Text-to-Speech \\nConversion System (ETTS) incorporating emotions through \\nRecurrent Neural Network (RNN) technology. Four \\nfundamental emotions, including 'happy,' 'sad,' 'anger,' and \\n'neutral,' are detected with high accuracy using the GRU \\nmodel within the RNN. The dataset is created by combining \\nexisting datasets like dailydialog, emotion-stimulus, and \\nisear. The model architecture comprises layers such as \\nEmbedded Layer, Bidirectional GRU Layer, and Dense \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='90e0e060-be4c-47e6-acdf-b2bd8d4e9136', embedding=None, metadata={'page_label': '2', 'file_name': 't2s_synthesis.pdf', 'file_path': '/kaggle/input/demo-db/t2s_synthesis.pdf', 'file_type': 'application/pdf', 'file_size': 1351269, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Layers. Achieving an accuracy of 86.77% on a combined \\ndataset, the system has promising applications in storytelling \\nand aiding visually impaired individuals. Future \\nimprovements may include standalone applications, \\nadditional emotions, and background music integration.  \\n \\n[2] Neural Speech Synthesis with Transformer Network.  \\nThis paper aiming to synthesize intelligible and natural \\naudios which are indistinguishable from human recordings. \\nTraditional TTS systems have two components: front-end \\nand back-end. Front-end is responsible for text analysis and \\nlinguistic feature extraction, such as word segmentation, part \\nof speech tagging, multi-word disambiguation and prosodic \\nstructure prediction; back -end is built for speech synthesis \\nbased on linguistic features from front-end, such as speech \\nacoustic parameter modeling, prosody modeling and speech \\ngeneration. In the past decades, concatenative and \\nparametric speech synthesis systems were mainstream \\ntechniques. However, both of them have complex pipelines, \\nand defining good linguistic features is often time-\\nconsuming and language specific, which requires a lot of \\nresource and manpower. Besides, synthesized audios often \\nhave glitches or instability in prosody and pronunciation \\ncompared to human speech, and thus sound unnatural. \\n \\n[3] Glow-TTS: A Generative Flow for Text-to-Speech via \\nMonotonic Alignment Search. \\nThis paper aim to introduces a novel text-to-speech (TTS) \\nsynthesis model called Glow-TTS. This model is designed \\nto generate mel-spectrograms conditioned on a monotonic \\nand non-skipping alignment between text and speech \\nrepresentations. Glow-TTS combines the best of both worlds \\nin terms of robustness, diversity, and controllability by \\nemploying both hard monotonic alignments and generative \\nflows. The paper presents the training and inference \\nprocedures of Glow-TTS, as well as an alignment search \\nalgorithm that removes the necessity of external aligners \\nfrom training. Additionally, it covers the architecture of all \\ncomponents of Glow-TTS, including the text encoder, \\nduration predictor, and flow-based decoder. Glow-TTS is \\npositioned as a diverse, robust, and fast TTS synthesis model \\nwith potential applications in AI voice assistant services, \\naudiobook services, advertisements, automotive navigation \\nsystems, and automated answering services. However, the \\npaper also acknowledges potential concerns related to the \\nabuse of TTS models for cyber crimes and the need for \\ncareful usage in critical domains such as news broadcast. \\nThe model's performance and capabilities make it a \\nsignificant contribution to the field of text-to-speech \\nsynthesis, offering a promising solution for natural and \\ncontrollable speech synthesis. \\n \\n[4] Grad-TTS: A Diﬀusion Probabilis Ɵc Model for Text -to-\\nSpeech. A novel approach to text-to-speech synthesis by \\nintroducing the Grad-TTS model, which employs a diffusion \\nprobabilistic model. In the realm of text-to-speech (TTS), \\ndiffusion models are likely utilized to depict the gradual \\nevolution of a system over time, possibly describing the \\nstep-by-step generation process of speech waveforms. The \\narchitecture of Grad-TTS would be a key focus, \\nencompassing the intricacies of the diffusion-based \\nprobabilistic model. Training the model would involve a \\ndataset comprising linguistic features (text) and \\ncorresponding acoustic features (speech waveforms), with \\nthe paper delving into the specifics of the training procedure, \\npotentially involving gradient-based optimization methods. \\nEvaluation metrics, both subjective (such as human listener assessments) and objective (related to speech quality), \\nwould likely be discussed to measure the performance of \\nGrad-TTS. \\n \\n[5] NaturalSpeech: End-to-End Text to Speech Synthesis \\nwith Human-Level Quality.  A focus on achieving high-\\nquality text-to-speech (TTS) synthesis through an end-to-\\nend approach, possibly with a model named NaturalSpeech. \\nThe paper likely explores a comprehensive system that takes \\ninput text and directly generates human-like speech, \\neliminating the need for intermediate steps. The emphasis is \\nlikely on achieving a level of quality comparable to human \\nspeech. The architecture and methodology of the \\nNaturalSpeech model would be central to the paper, \\ndetailing how the model processes and transforms text into \\nnatural-sounding speech. Training data composition and the \\ntraining procedure would also be crucial aspects, with \\npotential consideration for large datasets to capture diverse \\nlinguistic patterns. The evaluation section may include both \\nsubjective assessments, where human listeners rate the \\nquality of synthesized speech, and objective measures \\nassessing aspects like clarity, prosody, and naturalness.  \\n \\n[6]  Flowtron: An Autoregressive Flow-based Generative \\nNetwork for Text-to-Speech Synthesis. To introduces the \\nFlowtron model, which is designed for text-to-speech \\nsynthesis. Utilizing an autoregressive flow-based generative \\nnetwork, the model is likely structured to generate speech \\nwaveforms in a step-by-step manner, capturing intricate \\ndependencies in the input text. This approach may allow for \\nthe production of natural-sounding speech by considering \\nthe sequential nature of language. The paper likely \\nelaborates on the architecture and training procedure of \\nFlowtron, discussing how it processes text inputs and \\ntransforms them into high-quality speech outputs. \\nEvaluation metrics, both subjective and objective, are \\nprobably presented to assess the model's performance in \\nterms of naturalness and quality.  \\nFlow-based architecture: It employs invertible \\ntransformations to learn a flexible latent space, enabling \\nprecise control over speech output and efficient likelihood-\\nbased training. \\nAutoregressive generation: It produces mel spectrograms \\none frame at a time, ensuring temporal coherence and \\nnatural-sounding speech. \\nLatent space manipulation: It allows for fine-grained control \\nover speech characteristics like pitch, tone, speech rate, \\ncadence, and even speaker style, through manipulation of the \\nlatent space. \\n \\n[7]  FastSpeech: Fast, Robust, and Controllable Text to \\nSpeech. It involves proposing a novel TTS model called \\nFastSpeech. FastSpeech employs a non-autoregressive \\napproach, departing from traditional autoregressive \\nmethods, to enhance speed in generating speech. It utilizes a \\nsequence-to-sequence architecture with a feed-forward \\nTransformer, enabling parallelization for faster synthesis. \\nAdditionally, the model introduces a pitch prediction module \\nfor better prosody control.  the challenge of synthesizing \\nhigh-quality and natural-sounding speech from text in a fast, \\nrobust, and controllable manner. Traditional Text-to-Speech \\n(TTS) systems often face issues related to speed, robustness, \\nand control over speech synthesis, and this work aims to \\nimprove these aspects. the effectiveness of FastSpeech in \\nterms of speed, robustness, and controllability in comparison \\nto traditional autoregressive TTS models. The model \\nachieves competitive or superior performance in terms of \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='029ca91d-4c4b-40a8-85cb-0932085ad066', embedding=None, metadata={'page_label': '3', 'file_name': 't2s_synthesis.pdf', 'file_path': '/kaggle/input/demo-db/t2s_synthesis.pdf', 'file_type': 'application/pdf', 'file_size': 1351269, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='speech quality while significantly reducing synthesis time. \\nThe controllability aspect refers to the ability to manipulate \\nspeech attributes, such as pitch, with improved accuracy. \\nThe results are often evaluated using objective metrics and \\nsubjective evaluations, such as Mean Opinion Score (MOS), \\nto assess the quality of synthesized speech. \\n \\n[8] Text – To – Speech Synthesis (TTS). aimed at converting \\nwritten text into spoken language. The primary goal is to \\nfacilitate quick assimilation of textual information and \\nsupport reading development. The system targets individuals \\nwith physical impairments, including the deaf, dumb, blind, \\nand elderly, offering them a one-way communication \\ninterface. The methodology employs Object-Oriented \\nAnalysis and Development Methodology (OOADM) to \\npresent the system in a user-friendly manner. Additionally, \\nan Expert System is incorporated for internal operations, \\nmimicking human behavior by mapping knowledge into a \\nknowledge base. The core of the system is the Speech \\nSynthesis Module, which transforms arbitrary ASCII text \\ninto speech through the extraction of phonetic components, \\nmatching in a phonetic inventory, and generating acoustic \\nsignals for voice output. The chosen Free TTS speech \\nengine, programmed in JAVA, supports SAPI and JSAPI, \\nproviding effective control over speech signals. This \\napproach seeks to enhance accessibility and communication \\nfor those with physical limitations, contributing to the \\ndevelopment of inclusive technology solutions. \\n \\n[9] A Comprehensive Review-Based Study on Text-to-\\nSpeech Technologies. It aims to evaluate and compare \\ndifferent TTS technologies, including concatenative TTS, \\nformant synthesis TTS, and statistical parametric TTS. The \\nresearch focuses on understanding the advantages and \\nlimitations of these technologies in terms of naturalness of \\nvoice, system complexity, and suitability for different \\napplications. Additionally, the study aims to explore the \\nlatest advancements in TTS technology, such as neural TTS \\nand hybrid TTS. The problem statement includes the \\nchallenges faced by TTS systems, such as the lack of \\nemotional expressiveness and naturalness in synthesized \\nspeech. The methodology involves a comprehensive review \\nof existing TTS technologies. The study explores the \\nworking of TTS, including the types of voices (standard \\nvoice, neural voice, custom neural voice) and relevant \\nterminology like phoneme, prosody, and mel-spectrogram. \\nThe paper provides a high-level diagram of the TTS \\nstructure, outlining components such as the preprocessor, \\nencoder, decoder, and vocoder. The literature review section \\ncovers various studies on TTS, with a focus on Bangla \\nlanguage TTS systems. Different approaches, including unit \\nselection, diphone concatenation, and rule-based systems, \\nare discussed. \\n \\n[10] Image to Text to Speech Conversion Using Machine \\nLearning. It aims to involves the use of machine learning \\nalgorithms, particularly optical character recognition \\n(OCR), to recognize and extract text from images. The \\nproposed approach includes preprocessing steps such as \\nconverting images to grayscale, eliminating noise and non-\\ntext objects, and binarization. Advanced TTS technology is \\nintegrated into the system to convert the extracted text into \\nspeech. The authors mention the use of deep learning \\ntechniques for image captioning and established machine \\nlearning libraries and frameworks to implement and evaluate \\ntheir models. The study also presents a block diagram \\nillustrating the image-to-text-to-speech conversion process. the rapidly developing field of image-to-text-to-speech \\nconversion using machine learning. The key problem is \\nenabling the accurate extraction of text from images and \\nconverting it into speech. The authors emphasize the \\npotential of this technology to revolutionize information \\ninteraction by combining optical character recognition \\n(OCR) and text-to-speech (TTS) technologies. The \\napplication scenarios include making information more \\naccessible for people with visual impairments, students, \\ntourists, researchers, and musicians. The paper aims to \\ncontribute to improving the accuracy, efficiency, and \\nrobustness of image-to-text-to-speech conversion systems. \\n \\n[11]  Text To Speech Synthesizer. \\nIn this paper  involves a series of steps in the Text-To-Speech \\n(TTS) process. Firstly, text analysis is performed by \\nbreaking down input text into words and sentences. \\nFollowing this, text normalization transforms the text into a \\nspoken form. Grapheme-to-phoneme conversion assigns \\nphonetic transcription to each word, ensuring accurate \\npronunciation. The final step is speech synthesis, where a \\nwaveform corresponding to the input text is generated. The \\npaper introduces two models, namely the Common Form \\nmodel and the Grapheme-Phoneme Form model, to \\nelucidate the text-to-speech conversion process.  the problem \\nof converting text into speech using Text-To-Speech (TTS) \\ntechnology. It specifically focuses on the implementation of \\na computer-based system that can read any text aloud, \\nexploring the integration of Optical Character Recognition \\n(OCR) with speech synthesis technologies. The objective is \\nto create a cost-effective, user-friendly image-to-speech \\nconversion system using Python, enabling the \\ntransformation of both manually entered and scanned text \\ninto speech. The motivation behind this is the preference for \\nlistening to information rather than reading it, allowing for \\nmultitasking while consuming crucial data. \\n \\n[12]  An Efficient Approach for Text-to-Speech Conversion \\nUsing Machine Learning and Image Processing Technique. \\nThis novel approach aims to optimize the conversion \\nprocess, potentially resulting in faster and more accurate \\ntext-to-speech synthesis. The specific methodologies within \\nmachine learning and image processing are not detailed, but \\nthe paper likely explores their synergistic use for effective \\nand efficient text-to-speech conversion.  In this paper using \\nTwo approaches, maximally stable extensible region \\n(MSER) and grayscale conversion, are used for text \\ncharacter recognition. Geometric filtering combined with \\nstroke width transform (SWT) is employed for further \\nprocessing. Letter/alphabet grouping is performed to detect \\ntext sequences, fragmented into words. A 96 percent \\naccurate spell check is carried out using naive Bayes and \\ndecision tree algorithms, The primary focus is on assisting \\nvisually impaired individuals in accessing and \\nunderstanding textual content present in images, such as bus \\nnumbers, hotel names, newspapers, etc. \\n \\n[13] Text to Speech Synthesis: A Systematic Review, Deep \\nLearning Based Architecture and Future Research Direction.  \\na comprehensive examination of the landscape of text-to-\\nspeech synthesis. Through a systematic review, the paper \\nlikely surveys existing methodologies and approaches in the \\nfield, providing a holistic understanding of the \\nadvancements and challenges. The focus then shifts to a \\ndeep learning-based architecture, indicating a novel \\napproach grounded in deep learning techniques for \\nimproving the synthesis of speech from text. This could ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='8e67d7ed-04e8-4ae5-bc01-f945e5c98995', embedding=None, metadata={'page_label': '4', 'file_name': 't2s_synthesis.pdf', 'file_path': '/kaggle/input/demo-db/t2s_synthesis.pdf', 'file_type': 'application/pdf', 'file_size': 1351269, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"involve the utilization of neural networks, potentially \\naddressing limitations in prior methods. Moreover, the paper \\nlikely outlines future research directions, offering insights \\ninto areas where improvements and innovations can be made \\nin the realm of text-to-speech synthesis. For a nuanced \\nunderstanding of the paper's contributions, one would need \\nto delve into specific sections such as the systematic review \\nfindings, the proposed deep learning architecture, and the \\noutlined directions for future research.  TTS technology has \\nadvanced significantly, particularly with the integration of \\ndeep learning techniques. Further research is needed to \\nexplore the identified research directions to make TTS \\nsystems even more expressive, efficient, versatile, and \\nadaptable to diverse domains and real-time applications. \\n \\n[14]   Text to Speech Conversion using Raspberry – PI. \\nIt aims Optical Character Recognition (OCR) and Text-to-\\nSpeech Synthesizer (TTS) in a Raspberry Pi. The process \\ninvolves capturing an image using a camera, passing it to the \\nTTS unit installed in the Raspberry Pi, amplifying the TTS \\noutput with an audio amplifier, and then delivering it to a \\nspeaker. The OCR technology is used for text extraction \\nfrom color images, converting them into voice format. The \\nsystem comprises two main modules: image processing \\n(utilizing OCR) and voice processing (utilizing TTS). the \\nchallenge faced by visually impaired individuals in \\naccessing text-based information. With approximately 91% \\nof people being blind, the authors propose a low-cost \\nsolution utilizing Text-to-Speech (TTS) technology \\nintegrated with a Raspberry Pi. The system aims to convert \\ntext from images into voice or audio format, providing a \\nmeans for the blind to interact with computers through a \\nvocal interface. the successful detection of text on the image \\nand its conversion into an audio file. The system \\ndemonstrates the capability to convert both capital and small \\nletters, providing a comprehensive solution for text-to-\\nspeech conversion from images. \\n \\n[15] Text-To-Speech Synthesis System for Kannada  \\nLanguage. The primary objective is likely to create a \\ntechnology that converts written text in Kannada into spoken \\nlanguage, catering to the linguistic needs of Kannada \\nspeakers. The system's emphasis on Kannada is likely to \\nenhance accessibility and communication for users of this \\nspecific language. The methodology likely involves \\nlanguage-specific considerations for phonetics and linguistic \\nnuances, ensuring accurate and natural speech synthesis. \\nThis research contributes to the broader field of text-to-\\nspeech synthesis by addressing language-specific challenges \\nand requirements, facilitating improved communication and \\ninformation accessibility for Kannada speakers.  It \\nspecifically focuses on the challenges of synthesizing \\nnatural and emotionally expressive speech in Kannada. The \\ncritical factors considered include the selection of \\nappropriate speech units, prosody generation, and the \\nincorporation of emotional features to enhance the \\nnaturalness of the synthesized speech. providing valuable \\ninsights into the duration, pitch, and intensity dynamics in \\nemotional speech synthesis. \\n \\nIV. METHODOLOGY \\n \\nText-to-speech (TTS) synthesis encompasses a multi-step \\nprocess for converting written text into spoken words. The \\ninitial stage involves text preprocessing, addressing \\npunctuation and formatting issues, followed by linguistic analysis to grasp the text's structure and semantics. Prosody \\nmodeling becomes pivotal for infusing rhythm and \\nintonation into the synthesized speech, encompassing \\nparameters like pitch, duration, and amplitude. Additionally, \\nconsiderations for emotional and expressive elements are \\nintegrated to enhance naturalness. the subsequent phase \\ndelves into phonetic analysis, where the text is scrutinized at \\nthe phonetic level to identify the pronunciation of each word. \\nThis involves the mapping of text to phonemes, the \\nfundamental units of sound in a language, and an exploration \\nof coarticulation effects for more authentic speech \\ngeneration. Acoustic modeling plays a critical role in \\ntransforming phonetic and prosodic information into a \\nspeech signal representation. This representation can be \\nachieved through concatenative synthesis, involving the \\ncombination of pre-recorded speech segments, or parametric \\nsynthesis, where deep learning models like recurrent neural \\nnetworks or transformers generate speech waveforms based \\non learned parameters. \\n \\nVoice synthesis, a pivotal component, may involve the \\nutilization of a voice database containing recorded speech \\nsamples for concatenative synthesis. Alternatively, neural \\nnetwork models are employed for parametric synthesis. \\nPost-processing steps may include speech enhancement \\ntechniques to refine the quality of the synthesized speech, \\naddressing issues like noise reduction and improving overall \\nclarity. the synthesized speech is subjected to evaluation \\nthrough subjective listening tests and objective metrics \\ngauging factors such as intelligibility and naturalness. The \\niterative refinement process follows, incorporating user \\nfeedback and ongoing research to continually enhance \\nperformance and achieve a more human-like quality. \\nNotably, advancements in deep learning, particularly the \\nutilization of sophisticated neural network architectures, \\nhave significantly propelled the quality and naturalness of \\ntext-to-speech synthesis in recent years. \\n \\nV. RESULTS \\n \\nText-to-speech synthesis aims to produce speech output that \\nis natural, intelligible, and contextually appropriate. Ideally, \\nthe synthesized speech should closely resemble human \\nspeech patterns, capturing nuances of prosody, intonation, \\nand rhythm to achieve a natural sound. This includes \\nconveying emotional elements present in the input text, \\nmaking the output expressive and engaging. Phonetically \\naccurate pronunciation is crucial for clear and \\nunderstandable speech, considering variations in accents and \\nregional nuances. An effective TTS system should be \\nadaptable to different speaking styles, contexts, and \\nlanguages, providing versatility in its applications. Real-\\ntime synthesis capabilities, low latency, and high-quality \\naudio output contribute to a seamless user experience, \\nparticularly in interactive applications and voice-controlled \\nsystems. Customization options, allowing users to modify \\naspects of the synthesized voice, contribute to a personalized \\nand user-friendly experience. Despite the significant \\nadvancements driven by deep learning and neural network \\narchitectures, achieving perfect naturalness and prosody, \\nespecially in complex or emotionally expressive texts, \\nremains an ongoing challenge that researchers and \\ndevelopers continue to address. The ultimate goal is to \\ncontinually refine TTS technology to deliver high-quality, \\nhuman-like speech synthesis across diverse linguistic \\ncontexts and applications. \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='dd393e77-7755-45b3-8ee9-a030158105d5', embedding=None, metadata={'page_label': '5', 'file_name': 't2s_synthesis.pdf', 'file_path': '/kaggle/input/demo-db/t2s_synthesis.pdf', 'file_type': 'application/pdf', 'file_size': 1351269, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='VI. CONCLUSION \\n \\nText-to-Speech (TTS) technology has had a profound impact \\non accessibility, communication, and user experience. TTS \\nhas revolutionized the way individuals with visual \\nimpairments or reading difficulties access written content, \\nproviding them with equal opportunities to engage with \\ninformation, education, and entertainment. the \\nadvancements in TTS have led to more natural and high-\\nquality synthesized speech, making the listening experience \\nenjoyable and immersive. The availability of TTS in \\nmultiple languages has further broadened its reach, enabling \\npeople from diverse linguistic backgrounds to benefit from \\nthis technology. moreover, TTS systems are becoming \\nincreasingly personalized, allowing users to select voices \\nthat resonate with them or even create their own unique \\nvoices. The applications of TTS span across various \\nindustries, including assistive technologies, e-learning \\nplatforms, navigation systems, virtual assistants, \\naudiobooks, and entertainment media. As research \\ncontinues, the future of TTS holds tremendous potential for \\nfurther advancements, including enhanced naturalness, \\nexpressiveness, and adaptability.  \\n \\nVII. REFERENCES \\n \\n[1] Kim, Jaehyeon, et al. \"Glow-tts: A generative flow for \\ntext-to-speech via monotonic alignment search.\" Advances \\nin Neural Information Processing Systems 33 (2020): 8067-\\n8077 \\n \\n[2] Popov, Vadim, et al. \"Grad-tts: A diffusion probabilistic \\nmodel for text-to-speech.\" International Conference on \\nMachine Learning. PMLR, 2021 . \\n \\n[3] Li, Naihan, et al. \"Neural speech synthesis with \\ntransformer network.\" Proceedings of the AAAI conference \\non artificial intelligence. Vol. 33. No. 01. 2019. \\n \\n[4] Valle, Rafael, et al. \"Flowtron: an autoregressive flow-\\nbased generative network for text-to-speech \\nsynthesis.\" arXiv preprint arXiv:2005.05957 (2020). \\n \\n[5] Thu, Chaw Su Thu, and Theingi Zin. \"Implementation of \\ntext to speech conversion.\" International Journal of \\nEngineering Research & Technology (IJERT) 3.3 (2020). \\n \\n[6] Tan, Xu, et al. \"Naturalspeech: End-to-end text to speech \\nsynthesis with human-level quality.\" arXiv preprint \\narXiv:2205.04421 (2022). \\n [7] Ren, Yi, et al. \"Fastspeech: Fast, robust and controllable \\ntext to speech.\" Advances in neural information processing \\nsystems 32 (2019). \\n \\n [8] Nwakanma, Ifeanyi, Ikenna Oluigbo, and Izunna \\nOkpala. \"Text-to-speech synthesis.\" Advances in speech \\nprocessing. Springer, Cham, 2023. 123-152 \\n \\n[9] Ravi, D. J., and Sudarshan Patilkulkarni. \"Text-to-\\nspeech synthesis system for Kannada \\nlanguage.\" International Journal of Advanced Research in \\nComputer Science 2.1 (2021): 298-304. [10] Dr. S.A. Ubale1 , Girish Bhosale2 , Ganesh Nehe3 , \\nAvinash Hubale4 , Avdhoot Walunjkar5 . \"A Review on Text-\\nto-Speech Converter.\" Proc. ICSLP. 2022. \\n \\n[11] Sneha Tamboli1 , Pratiksha Raut1 , Lavkush \\nSategaonkar1 , Anjali Atram1 , Shubham Kawane1 , Prof. V. \\nK. Barbudhe. “A review paper of Text to speech Convertor”,  \\nInternational Journal of Research Publication and Reviews, \\nVol 3, no 5, pp 3807-3810, May 2022. \\n \\n[12] Chowdhury, Md Jalal Uddin, and Ashab Hussan. \"A \\nreview-based study on different Text-to-Speech \\ntechnologies.\" arXiv preprint arXiv:2312.11563 (2023). \\n \\n[13] Shastri, Swaroopa, and Shashank Vishwakarma. \"An \\nEfficient Approach for Text-to-Speech Conversion Using \\nMachine Learning and Image Processing Technique.\" \\n \\n[14] Phutak, Vinaya, et al. \"Text to speech conversion using \\nraspberry-pi.\" International Journal of Innovative Science \\nand Research Technology 4.2 (2019): 1-3. \\n \\n[15] Khanam, Fahima, et al. \"Text to Speech Synthesis: A \\nSystematic Review, Deep Learning Based Architecture and \\nFuture Research Direction.\" Journal of Advances in \\nInformation Technology Vol 13.5 (2022). \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='d05e65bc-9722-488b-a13c-957c4038895a', embedding=None, metadata={'page_label': '1', 'file_name': 't2speech.pdf', 'file_path': '/kaggle/input/demo-db/t2speech.pdf', 'file_type': 'application/pdf', 'file_size': 281862, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n A review -based  study  on different  Text-to-Speech  \\ntechnologies  \\n \\n \\nMd. Jalal Uddin  Chowdhury,  Ashab  Hussan  \\nLeading  Univ ersity,  jalalchy101,  ashabhtanim @gmail.com  \\n \\n \\n \\nAbstract  - This  research  paper  presents  a comprehensive  \\nreview -based  study  on various  Text -to-Speech  (TTS)  \\ntechnologies.  TTS  technology  is an important  aspect  of \\nhuman -computer  interaction,  enabling  machines  to \\nconvert  written  text into audible  speech.  The paper  \\nexam ines the different  TTS  technologies  available,  \\nincluding  concatenative  TTS,  formant  synthesis  TTS,  and \\nstatistical  parametric  TTS.  The study  focuses  on \\ncomparing  the advantages  and limitations  of these  \\ntechnologies  in terms  of their  naturalness  of voice,  the \\nlevel  of complexity  of the system,  and their  suitability  for \\ndifferent  applications.  In addition,  the paper  explores  the \\nlatest  advancements  in TTS  technology,  including  neural  \\nTTS  and hybrid  TTS.  The findings  of this research  will \\nprovide  valuable  insights for researchers,  developers,  and \\nusers  who want  to understand  the different  TTS  \\ntechnologies  and their  suitability  for specific  applications.  \\n \\nIndex  Terms  – Natural  Language  Processing,  Text-to-Speech . \\nINTRODUCTION  \\nNowadays,  we are living  in the digital era. Every  staff \\nassociated  with our life is getting  digital.  Almost  every  \\nsmartphone  has a smart  assistant  that can speak  and \\ncommunicate  like a human.  Speech  recognition  is one of the \\ntechnologies  used in those  smart  assistants.  Text-to-Speech  is \\na part of speech  recognition.  Text-to-speech  (TTS)  is a natural  \\nlanguage  modeling  approach  that converts  text units  into \\nspeech  units  for audio  presentation.  There  are numerous  \\ntechnologies  used in Text-to-Speech.  Many  programming  \\nlanguages  are used in these  technologies.  Python,  a \\nprogramming  language  mostly  used in Text-to-Speech  \\ntechnology.  There  are many  Python  library  e.g gTTS,  pyttsx3,  \\npaddlespeech.  But everyone's  performance  is not the same.  \\nOur thesis  is about  measuring  the efficiency  of various   Text-\\nto-Speech  technology  in various  aspects . \\nI. How  to Works  TTS \\nText-to-speech  converts  text into human -like speech,  along  \\nwith the ability  to create  a unique,  custom  voice.   \\nA. Type  of Voice:  \\nStandard  voice  : Standard  voice  is the simplest  and most  \\ncost-effective  type of voice.  In the past few years  standard  \\nvoice  has improved  considerably  to provide  a human -like voice  in multiple  regional  dialects,  such as Hindi  or Irish \\nEnglish.  Regional  dialects  provide  greater clarity  of \\npronunciation  of region -specific  words  or phrases,  making  \\nfor more  understandable  and accessible  accents.   \\nNeural  Voice:  Neural  voice  is a new type of synthesized  \\nspeech  that’s  nearly  indistinguishable  from  human  \\nrecordings.  Powered  by deep  neural  networks,  neural  voices  \\nsound  more  natural  than standard  voices  by producing  \\nhuman -like speech  patterns,  such as stress  and loudness  of \\nindividual  words.  Because  of this human -like speech,  you \\nend up with a more  precise  articulation  of words,  along  with \\na significant  reduction  in listening  fatigue  when  users  \\ninteract  with AI systems.  \\nCustom  Neural  Voice:  Custom  neural  voice  uses your own \\naudio  data to create  a one-of-a-kind customized  synthetic  \\nvoice.  Custom  neural  voice  has the deepest  level  of voice \\npersonalization,  with realistic  speech  that can be used to \\nrepresent  brands,  personify  machines,  and allow  users  to \\ninteract  with applications  conversationally.   \\nB. Some  Terminology   \\nPhoneme:  A phoneme  is the smallest  unit of sound  that \\nmakes  a word’s  pronunciation  and meaning  different  from  \\nanother  word.  \\nProsody:  The patterns  of rhythm  and sound  used in poetry.   \\nMel-spectrogram:  It is derived  by applying  a non-linear  \\ntransformation  to the frequency  axis of short -time Fourier  \\ntransform  (STFT)  of audio,  to reduce  the dimensionality.  It \\nemphasizes  details  in low frequencies  which  are very \\nimportant  to distinguish  speech  and de-emphasizes  details  in \\nhigh frequencies  which  usually  are noise.  \\nText -To-Speech  (TTS)  Structure  \\n \\n \\nFig. 1 : Text-to-speech  structure  \\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='224a835d-0627-41c2-987d-ef41d263153c', embedding=None, metadata={'page_label': '2', 'file_name': 't2speech.pdf', 'file_path': '/kaggle/input/demo-db/t2speech.pdf', 'file_type': 'application/pdf', 'file_size': 281862, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n This is a high-level  diagram  of different  components  used in \\nthe TTS system.  The input  to our model  is text, which  passes  \\nthrough  several  blocks  and eventually  is converted  to audio.   \\n \\nPreproces sor \\n \\n● Tokenize : Tokenize  a sentence  into words  \\n● Phonemes/Pronunciation:  It breaks  input  text into \\nphonemes,  based  on their pronunciation.  For \\nexample,  “Hello,  Have  a good  day”  converts  to HH \\nAH0  L OW1,  HH AE1 V AH0  G UH1  D D EY1.  \\n● Phoneme  duration:  Represents  the total time taken  \\nby each phoneme  in the audio.  \\n● Pitch : Key feature  to convey  emotions,  it greatly  \\naffects  the speech  prosody.  \\n● Energy : Indicates  frame -level  magnitude  of mel-\\nspectrograms  and directly  affects  the volume  and \\nprosody  of speech.  \\nThe Linguistic  feature  only contains  phonemes.  Energy,  \\npitch,  and duration  are actually  used to train the energy  \\npredictor,  the pitch  predictor,  and the duration  predictor  \\nrespectively  which  are used by the model  to get a more  natural  \\noutput.  \\n \\n \\nEncoder  \\n \\n \\n \\nThe encoder  inputs  Linguistic  features  (Phonemes)  and \\noutputs  an n-dimensional  embedding.  This embedding  \\nbetween  the encoder  and decoder  is known  as the latent  \\nfeature.  Latent  features  are crucial  because,  other  features  like \\nspeaker  embedding  are concat enated  with these  and passed  to \\nthe decoder.  Furthermore,  the latent  features  are also used for \\nthe prediction  of energy,  pitch,  and duration  which  in turn \\nplay a crucial  role in controlling  the naturalness  of the audio.  \\n \\nDecoder   \\nThe decoder  is used to convert  information  embedded  in the \\nLatent  processed  feature  to the Acoustic  feature  i.e. Mel-\\nspectrogram.  \\n \\nVocoder  \\n \\nIt converts  the Acoustic  feature  (Mel -spectrogram)  to \\nwaveform  output  (audio).  It can be done  using  a mathematical  \\nmodel  like Griffin  Lim or we can also train a neural  network  \\nto learn  the mapping  from  mel-spectrogram  to waveforms.  In \\nreality,  learning -based  methods  usually  outperform  the \\nGriffin  Lim method.  \\n \\nSo instead  of directly  predicting  waveform  using  the decoder,  \\nwe split this complex  and sophisticated  task into two stages,  \\nfirst predicting  mel-spectrogram  from  Latent  processed  \\nfeatures  and then generating  audio  using  mel-spectrogram.  \\n  \\nLITERATURE  REVIEW  \\nDesigning  an effective  text-to-speech  synthesis  system  is \\nquite  difficult.  Building  a whole  TTS system  requires  \\ncompleting  several  steps,  including  normalizing  text, \\nconverting  text to phonemes,  identifying  prosodic  emotional  \\ncontent,  and generating  speec h. \\nSpeech  synthesis  for different  languages  has already  been  the \\nsubject  of many  research  proposals.  Before  electronic  signal  \\nprocessing  was invented,  some  early  scientists  tried to make  \\nmachines  that could  mimic  human  speech.  \\n \\nA Unit Selection  approach  for the text-to-speech  synthesis  \\nusing  Syllabic  was presented  in [1]. In this paper,  They  select  \\nsyllables  as their unit- hence  this was the first syllable  based  \\nText to Speech  conversion  system  for Bangla  language.  In \\nthis System,  It is necessary  to conduct  a substantial  amount  of \\ntesting  with an even  larger  text corpus  than the one they \\nutilized  as an experimental  text corpus.  \\n \\nThe research  by F. Alam  and colleagues  resulted  in the \\ndevelopment  of a speech  synthesizer  for the Bangla  \\nlanguage[2,3].  The diphon e concatenation  method  was used \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='758eff33-a888-4b4f-ab49-327a5e3b093e', embedding=None, metadata={'page_label': '3', 'file_name': 't2speech.pdf', 'file_path': '/kaggle/input/demo-db/t2speech.pdf', 'file_type': 'application/pdf', 'file_size': 281862, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n to make  this system.  It needs  a dictionary  that tells it how to \\nsay words  so that it can talk. There  are 93000  entries  in the \\ndictionary  [3]. The proposed  system  makes  voice  data for a \\nfestival  and adds support  for the Bangl a language  to the \\nfestival  using  its embedded  scheme  scripting  interface.  It \\nturns  Bangla  Unicode  text into ASCII  text based  on the \\nBangla  phone  set. But there  is no explanation  of how the \\nprocess  of transliteration  works.  Also,  there  is no information  \\nabout how the letter -to-sound  (LTS)  rules  for words  that aren't \\nin the lexicon  were  made.  \\n \\nIn [4], the author  showed  how a Bangla  Text-to-Speech  (TTS)  \\nsystem  was designed  and built from  the raw level  without  \\nusing  third -party  speech  synthesis  tools.  For build ing the \\nsystem,  they were  looked  at from  two different  angles:  one \\nbased  on phonemes  and the other  on syllables.This  study  was \\nconducted  on a very raw level,  and the researchers  used \\nrecordings  of their own voices  to produce  phonemes  and \\nsyllables.  The syllable -based  method  showed  higher  quality  \\nspeech  than the phoneme -based  method.  But, limited  syllable  \\nand phoneme  data were  used for the development  process.  \\nIn [5], the author  used a concatenative  synthesis  technique  to \\nmake  the system's  speech  sound  natur al.In this paper,  They  \\nproposed  a system  that converted  Bangla  text to Romanized  \\ntext based  on Bangla  graphemes  set and by developing  a \\nbunch  of romanization  rules.They  used the MBROLA  \\ndiphone  database  and did not develop  their own \\ndatabase.Also,  The sound  quality  is not particular ly natural  in \\nits presentation.  \\n \\nIn [6], they present  FastPitch,  a fully -parallel  text-to-speech  \\nmodel  based  on FastSpeech,  conditioned  on fundamental  \\nfrequency  contours.  Pitch  contours  are predicted  by the model  \\nduring  inference.  The generated  speech  can be made  more  \\nexpressive,  better  match  the meaning  of the utterance,  and \\nultimately  more  interesting  to the audience  by changing  these  \\npredictions.  \\n \\nIn [7], the authors  of the paper  propose  LightSpeech,  which  \\nleverages  neural  archi tecture  search  (NAS)  to automatically  \\ndesign  more  lightweight  and efficient  models  based  on \\nFastSpeech.  They  meticulously  developed  a fresh  search  \\nspace  that includes  a variety  of lightweight  and potentially  \\nefficient  architectures  after thoroughly  profili ng the \\ncomponents  of the current  FastSpeech  model.  Then,  within  \\nthe search  space,  NAS  is used to automatically  find well-\\nperforming  architectures.  The model  developed  by their \\nmethod,  according  to experiments,  achieved  a 15x model  \\ncompression  ratio and a 6.5x inference  speedup  on the CPU  \\nwhile  maintain ing a comparable  voice  quality.  \\n \\nIn [8], the authors  made  a rule-based  system  for normalizing  \\nBangla  text instead  of a decision  tree and a decision  list for \\nambiguous  tokens.In  this paper,  a lexical  analyzer  was \\ndeveloped  to tokenize  each NSW(Non  Standard  Words)  by \\nusing  a regular  expression  and the tool JFlex[9].  This was \\ndone  based  on semiotic  classes.  The main  thing  that the work  was that it was done  in sequences  of tokenization,  token  \\nclassification,  token  sense  disambiguation,  and standard  word  \\ngeneration.  This work  will be useful  in the future  because  it \\ncombines  TTS and Speech  Recognition  and compares  the \\nways  that rule-based  systems  and other  classific ation  systems  \\nhandle  ambiguity.  \\n \\nIn[10],The  authors  developed  an audio  programming  tool for \\nblind  and vision -impaired  people  to learn  programming  that \\nis based  on text-to-speech  technology.  In this paper,  they \\ndemonstrate  how users  who use the tool can edit programs,  \\ncompile,  debug,  and run them.  The authors  mentioned  that \\nthese  levels  can all be voiced.  As a programming  language,  \\nthey use C# for evaluation,  and VisualStudio.NET  is used to \\ncreate  the tool. Evaluations  have  demonstrated  that the \\nprogramming  tool can support  the implementation  of software  \\nappli cations  by blind  and vision -impaired  individuals  and the \\nachievement  of equality  of access  and opportunity  in \\ninformation  technology  education.  To communicate  with a \\ncomputer,  vision -impaired  people  liked  to use mouse  events  \\nand blind  people  liked  to use keyboards  with shortcut  keys \\ndefined  in JAWS.  This means  there’s  not any inbuilt  or \\nintuitive  systematic  approach  to handle  the interaction  with \\ncomputers.  \\n \\nA diphone -based  concatenative  technique  was utilized  by the \\nauthors  in the development  of a speech  synthesizer  for the \\nBangla  language  [11].In  addition  to this unique  collection  of \\nwords,  the tokenization  of null-modified  characters  has been  \\npresented  in this study.  This is an important  and, to put it \\nmildly,  a tough  task for a text-to-speech  program  (TTS). \\n \\nFrom  the perception  of the authors,  despite  the fact that over \\n1.6 billion  Muslims  live in the world  and that Arabic  is spoken  \\nby millions  of people  as an official  language  in 24 different  \\nnations,  it has received  less attention  than other  languages  \\n[12]. These  considerations  highlight  the necessity,  from  the \\npoint  of view  of the authors,  for an Arabic  TTS that could  be \\nof the highest  quality,  be lightweight,  and be absolutely  free. \\nA rule-based  system  with an exception  dictionary  for words  \\nthat don't  follow  the letter -to-phoneme  rules  might  be a much  \\nmore  sensible  approach  since  the vowelized  written  text of \\nArabic  bears  the pronunciation  rules  with few exceptions  \\nfrom  their perspective.  This study  developed  a rule-based  \\ntext-to-speech  hybrid  synthesis  system  that combined  formant  \\nand concatenation  approaches  to produce  speech  that sounds  \\nnatural  enough.  But for the lack of significant  stressed  \\nsyllables  and intonation,  the overall  system  might  not perform  \\nintuitively  as well as handle  the differentiati on in different  \\narabic  accents.  \\n \\n \\nCONCLUSION  AND  FUTURE  DIRECTION  \\nThis review -based  study  has examined  different  Text-to-\\nSpeech  (TTS)  technologies  and highlighted  their advantages  \\nand limitations.  The study  has provided  an overview  of the \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='5280abd1-540a-481d-92af-d343fc0340e5', embedding=None, metadata={'page_label': '4', 'file_name': 't2speech.pdf', 'file_path': '/kaggle/input/demo-db/t2speech.pdf', 'file_type': 'application/pdf', 'file_size': 281862, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n basic  functionalities  of TTS systems  and has shown  how they \\nhave  evolved  over time,  from  rule-based  systems  to neural -\\nbased  models.  The study  has also explored  the impact  of TTS \\non different  industries,  including  education,  entertainment,  \\nand healthcare.  One of the key findings  of this study  is that \\nthe recent  advancements  in deep  learning  have  significantly  \\nimproved  the quality  of TTS systems.  However,  there  are still \\nseveral  challenges  that need  to be addressed,  such as the lack \\nof emotional  expressiveness  and naturalness  in synthesized  \\nspeech,  which  can affect  the user experience.  In terms  of \\nfuture  directions,  further  research  is needed  to improve  the \\nperformance  of TTS systems  in terms  of naturalness,  \\nexpressiveness,  and intonation.  This can be achieved  by \\ndeveloping  more  advanced  algorithms  that can capture  the \\nnuances  of human  speech  and emotions.  Additionally,  more  \\nstudies  are needed  to evaluate  the effectiveness  of TTS in \\nvarious  applications,  such as language  learning  and speech  \\ntherapy.  Overall,  TTS technology  has the potential  to \\nrevolutionize  the way we communicate  and interact  with \\nmachines.  As the technology  continues  to evolve,  it will \\nbecome  increasingly  important  to address  the limitations  and \\nchallenges  of TTS to ensure  that it can be used to its full \\npotential.  \\nREFERENCES  \\n[1] Sadeque,  F. Y., Yasar,  S., & Islam,  M. M. (2013,  May).  Bangla  text to \\nspeech  conversion:  A syllabic  unit selection  approach.  In 2013  International  \\nConference  on Informatics,  Electronics  and Vision  (ICIEV)  (pp. 1-6). IEEE.  \\n \\n[2] Firoj  Alam,  Promila  Kanti  Nath,  Mumit  Khan  (2007  ’Text  to speech  for \\nBangla  language  using  festival’,  BRAC  University.  \\n \\n[3] Firoj  Alam,  Promila  Kanti  Nath,  Mumit  Khan  (2011)  ‘Bangla  text to \\nspeech  using  festival’,Conference  on human  language  technology  for \\ndevelopment,  pp.154 -161 \\n \\n[4] Arafat,  M. Y., Fahrin,  S., Islam,  M. J., Siddiquee,  M. A., Khan,  A., \\nKotwal,  M. R. A., & Huda,  M. N. (2014,  December).  Speech  synthesis  for \\nbangla  text to speech  conversion.  In The 8th International  Conference  on \\nSoftware,  Knowledge,  Information  Management  and Applications  (SKIMA  \\n2014)  (pp. 1-6). IEEE.  \\n \\n \\n[5] Ahmed,  K. M., Mandal,  P., & Hossain,  B. M. (2019).  Text to Speech  \\nSynthesis  for Bangla  Language.  International  Journal  of Information  \\nEngineering  and Electronic  Busines s, 12(2),  1. \\n \\n[6] A. Łańcucki,  \"Fastpitch:  Parallel  Text-to-Speech  with Pitch  Prediction,\"  \\nICASSP  2021  - 2021  IEEE  International  Conference  on Acoustics,  Speech  \\nand Signal  Processing  (ICASSP),  2021,  pp. 6588 -6592,  doi: \\n10.1109/ICASSP39728.2021.9413889.  \\n \\n[7] R. Luo et al., \"Lightspeech:  Lightweight  and Fast Text to Speech  with \\nNeural  Architecture  Search,\"  ICASSP  2021  - 2021  IEEE  International  \\nConference  on Acoustics,  Speech  and Signal  Processing  (ICASSP),  2021,  pp. \\n5699 -5703,  doi: 10.1109/ICASSP39728.2021.94 14403.  \\n \\n \\n[8] Firoj  Alam,  S.M.  Murtoza  Habib,  Mumit  Khan,  “Text  normalization  \\nsystem  for Bangla,”  Proc.  of Conf.  on Language  and Technology,  Lahore,  \\npp. 22-24, 2009.  \\n \\n[9] Elliot  Berk,  JFlex  - The Fast Scanner  Generator  for Java,  2004,  version  \\n1.4.1,  http:// jflex.de   \\n [10] Tran,  D., Haines,  P., Ma, W., & Sharma,  D. (2007,  September).  Text-\\nto-speech  technology -based  programming  tool. In International  Conference  \\nOn Signal,  Speech  and Image  Processing . \\n \\n[11] Rashid,  M. M., Hussain,  M. A., & Rahman,  M. S. (2010).  Text \\nnormalization  and diphone  preparation  for bangla  speech  synthesis.  Journal  \\nof Multimedia , 5(6), 551. \\n \\n[12] Zeki,  M., Khalifa,  O. O., & Naji,  A. W. (2010,  May).  Development  of \\nan Arabic  text-to-speech  system.  In International  Conference  on Computer  \\nand Communication  Engineering  (ICCCE\\'10)  (pp. 1-5). IEEE.  \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='6751c01c-5aa2-4342-8184-b4f9527bcdbb', embedding=None, metadata={'page_label': '1', 'file_name': 't2v_visual_impaired.pdf', 'file_path': '/kaggle/input/demo-db/t2v_visual_impaired.pdf', 'file_type': 'application/pdf', 'file_size': 1114479, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' International Journal of Applied Information Systems (IJAIS) – ISSN  : 2249 -0868   \\nFoundation of Computer Sci ence FCS, New York, USA  \\nVolume 7– No. 2, April 2014  – www.ijais.org  \\n \\n25 \\nDesign  and Implementation of Text To Speech  \\nConversion for Visually Impaired People  \\nItunuoluwa Isewon * \\nDepartment of Computer and \\nInformation Sciences  \\nCovenant University  \\nPMB 1023, Ota, Nigeria  \\n \\n* Corresponding Author  Jelili Oyelade  \\nDepartment of Computer and \\nInformation Sciences  \\nCovenant University  \\nPMB 1023, Ota, Nigeria   \\n \\n  Olufunke Oladipupo  \\nDepartment of Computer and \\nInformation Sciences  \\nCovenant University  \\nPMB 1023, Ota, Nigeria   \\n \\n \\nABSTRACT  \\nA Text -to-speech synthesizer is an application that converts \\ntext into spoken word, by analyzing and processing the text \\nusing Natural Language Processing (NLP) and then using \\nDigital Signal Processing (DSP) technology to convert this \\nprocessed text into synthesized speech representation of the \\ntext. Here, we develop ed a useful text -to-speech synthesizer in \\nthe form of a simple application that converts inputted text \\ninto synthesized speech and reads out to the user which can \\nthen be saved as an mp3 .file. The development of a text to \\nspeech synthesizer will be of great help to people with visual \\nimpairment  and make making through large volume of text \\neasier.  \\nKeywords   \\nText-to-speech synthesis, Natural Language Processing, \\nDigital Signal Processing  \\n1. INTRODUCTION  \\nText-to-speech synthesis -TTS - is the automatic conversion \\nof a text into speech that resembles,  as closely as possible, a \\nnative speaker of the language reading that text. Text -to-\\nspeech synthesizer (TTS) is the technology which lets \\ncomputer speak to you. The TTS system gets the text as the \\ninput and then a computer algorithm which called TTS engin e \\nanalyses the text, pre -processes the text and synthesizes the \\nspeech with some mathematical models. The TTS engine \\nusually generates sound data in an audio format as the output.  \\n \\nThe text -to-speech (TTS) synthesis procedure consists of two \\nmain phases. T he first is text analysis, where the input text is \\ntranscribed into a phonetic or some other linguistic \\nrepresentation, and the second one is the generation of speech \\nwaveforms, where the output is produced from this phonetic \\nand prosodic information. Thes e two phases are usually called \\nhigh and low -level synthesis [1]. A simplified version of this \\nprocedure is presented in figure  1 below. The input text might \\nbe for example data from a word processor, standard ASCII \\nfrom e -mail, a mobile text -message, or s canned text from a \\nnewspaper. The character string is then pre -processed and \\nanalyzed into phonetic representation which is usually a string \\nof phonemes with some additional information for correct \\nintonation, duration, and stress. Speech sound is finally \\ngenerated with the low -level synthesizer by the information \\nfrom high -level one. The artificial production of speech -like \\nsounds has a long history, with documented mechanical \\nattempts dating to the eighteenth century.   \\nFigure 1: A simple but general functional diagram of a \\nTTS system . [2] \\n2. OVERVIEW OF SPEECH SYNTHESIS  \\nSpeech synthesis can be described as artificial production of \\nhuman speech [3]. A computer system used for this purpose is \\ncalled a speech synthesizer, and can be im plemented in \\nsoftware or hardware. A text -to-speech (TTS) system converts \\nnormal language text into speech [4].  Synthesized speech can \\nbe created by concatenating pieces of recorded speech that are \\nstored in a database. Systems differ in the size of the s tored \\nspeech units; a system that stores phones or diphones provides \\nthe largest output range, but may lack clarity. For specific \\nusage domains, the storage of entire words or sentences \\nallows for high -quality output. Alternatively, a synthesizer \\ncan incor porate a model of the vocal tract and other human \\nvoice characteristics to create a completely \"synthetic\" voice \\noutput [5]. The quality of a speech synthesizer is judged by its \\nsimilarity to the human voice and by its ability to be \\nunderstood. An intellig ible text -to-speech program allows \\npeople with visual impairments or reading disabilities to listen \\nto written works on a home computer.  \\nA text -to-speech system (or \"engine\") is composed of two \\nparts: [6] a front -end and a back -end. The front -end has two \\nmajor tasks. First, it converts raw text containing symbols like \\nnumbers and abbreviations into the equivalent of written -out \\nwords. This process is often called text normalization, pre -\\nprocessing, or tokenization. The front -end then assigns \\nphonetic transc riptions to each word, and divides and marks \\nthe text into prosodic units, like phrases, clauses, and \\nsentences. The process of assigning phonetic transcriptions to \\nwords is called text -to-phoneme or grapheme -to-phoneme \\nconversion. Phonetic transcriptions and prosody information \\ntogether make up the symbolic linguistic representation that is \\noutput by the front -end. The back -end—often referred to as \\nthe synthesizer —then converts the symbolic linguistic \\nrepresentation into sound. In certain systems, this par t \\nincludes the computation of the target prosody (pitch contour, \\nbrought to you by CORE View metadata, citation and similar papers at core.ac.uk\\nprovided by Covenant University Repository', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='7b3584d8-9a31-4285-b88f-b9c8220b7358', embedding=None, metadata={'page_label': '2', 'file_name': 't2v_visual_impaired.pdf', 'file_path': '/kaggle/input/demo-db/t2v_visual_impaired.pdf', 'file_type': 'application/pdf', 'file_size': 1114479, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' International Journal of Applied Information Systems (IJAIS) – ISSN  : 2249 -0868   \\nFoundation of Computer Sci ence FCS, New York, USA  \\nVolume 7– No. 2, April 2014  – www.ijais.org  \\n \\n26 \\nphoneme durations), [7] which is then imposed on the output \\nspeech.  \\nThere are different ways to perform speech synthesis. The \\nchoice depends on the task they are used for, but the most \\nwidely  used method is Concatentive Synthesis, because it \\ngenerally produces the most natural -sounding synthesized \\nspeech. Concatenative synthesis is based on the concatenation \\n(or stringing together) of segments of recorded speech. There \\nare three major sub -types of concatenative synthesis [8]:  \\nDomain -specific Synthesis: Domain -specific synthesis \\nconcatenates pre -recorded words and phrases to create \\ncomplete utterances. It is used in applications where the \\nvariety of texts the system will output is limited to a p articular \\ndomain, like transit schedule announcements or weather \\nreports. [9] The technology is very simple to implement, and \\nhas been in commercial use for a long time, in devices like \\ntalking clocks and calculators. The level of naturalness of \\nthese syst ems can be very high because the variety of sentence \\ntypes is limited, and they closely match the prosody and \\nintonation of the original recordings. Because these systems \\nare limited by the words and phrases in their databases, they \\nare not general -purpose  and can only synthesize the \\ncombinations of words and phrases with which they have \\nbeen pre -programmed. The blending of words within naturally \\nspoken language however can still cause problems unless \\nmany variations are taken into account. For example, in non-\\nrhotic dialects of English the \"r\" in words like \"clear\" /ˈkl ɪə/ is \\nusually only pronounced when the following word has a \\nvowel as its first letter (e.g. \"clear out\" is realized as \\n/ˌklɪəɾˈʌʊt/) [10]. Likewise in French, many final consonants \\nbecome no  longer silent if followed by a word that begins \\nwith a vowel, an effect called liaison. This alternation cannot \\nbe reproduced by a simple word -concatenation system, which \\nwould require additional complexity to be context -sensitive. \\nThis involves recording  the voice of a person speaking the \\ndesired words and phrases. This is useful if only the restricted \\nvolume of phrases and sentences is used and the variety of \\ntexts the system will output is limited to a particular domain \\ne.g. a message in a train station , whether reports or checking a \\ntelephone subscriber’s account balance. .  \\nUnit Selection Synthesis:  Unit selection synthesis uses large \\ndatabases of recorded speech. During database creation, each \\nrecorded utterance is segmented into some or all of the \\nfollowing: individual phones, diphones, half -phones, \\nsyllables, morphemes, words, phrases, and sentences. \\nTypically, the division into segments is done using a specially \\nmodified speech recognizer set to a \"forced alignment\" mode \\nwith some manual correction a fterward, using visual \\nrepresentations such as the waveform and spectrogram. [11]. \\nAn index of the units in the speech database is then created \\nbased on the segmentation and acoustic parameters like the \\nfundamental frequency (pitch), duration, position in the \\nsyllable, and neighboring  phones. At runtime, the desired \\ntarget utterance is created by determining the best chain of \\ncandidate units from the database (unit selection). This \\nprocess is typically achieved using a specially weighted \\ndecision tree.  \\nUnit selection provides the greatest naturalness, because it \\napplies only a small a90 -mount of digital signals processing \\n(DSP) to the recorded speech. DSP often makes recorded \\nspeech sound less natural, although some systems use a small amount of signal proce ssing at the point of concatenation to \\nsmooth the waveform. The output from the best unit -selection \\nsystems is often indistinguishable from real human voices, \\nespecially in contexts for which the TTS system has been \\ntuned. However, maximum naturalness typi cally require unit -\\nselection speech databases to be very large, in some systems \\nranging into the gigabytes of recorded data, representing \\ndozens of hours of speech. [12]. Also, unit selection \\nalgorithms have been known to select segments from a place \\nthat results in less than ideal synthesis (e.g. minor words \\nbecome unclear) even when a better choice exists in the \\ndatabase. [13].  \\nDiphone Synthesis:  Diphone synthesis uses a minimal speech \\ndatabase containing all the diphones (sound -to-sound \\ntransitions) occu rring in a language. The number of diphones \\ndepends on the phonotactics of the language: for example, \\nSpanish has about 800 diphones, and German about 2500. In \\ndiphone synthesis, only one example of each diphone is \\ncontained in the speech database. At runt ime, the target \\nprosody of a sentence is superimposed on these minimal units \\nby means of digital signal processing techniques such as \\nlinear predictive coding, PSOLA[12] or MBROLA. [14] The \\nquality of the resulting speech is generally worse than that of \\nunit-selection systems, but more natural -sounding than the \\noutput of formant synthesizers. Diphone synthesis suffers \\nfrom the sonic glitches of concatenative synthesis and the \\nrobotic -sounding nature of formant synthesis, and has few of \\nthe advantages of eit her approach other than small size. As \\nsuch, its use in commercial applications is declining, although \\nit continues to be used in research because there are a number \\nof freely available software implementations [15].  \\n3. Structur e of A Text -To-Speech \\nSynthesi zer System   \\nText-to-speech synthesis takes place in several steps. The TTS \\nsystems get a text as input, which it first must analyze  and \\nthen transform into a phonetic description. Then in a further \\nstep it generates the prosody. From the information now \\navailable, it can produce a speech signal.  \\nThe structure of the text -to-speech synthesizer can be broken \\ndown into major modules:  \\n\\uf0b7 Natural Language Processing (NLP) module:  It \\nproduces a phonetic transcription of the text read, \\ntogether with prosody.  \\n\\uf0b7 Digital  Signal Processing (DSP) module:  It \\ntransforms the symbolic information it receives \\nfrom NLP into audible and intelligible speech.  \\nThe major operations of the NLP module are as follows:  \\n\\uf0b7 Text Analysis:  First the text is segmented into \\ntokens. The token -to-word conversion creates the \\northographic form of the token. For the token “Mr” \\nthe orthographic form “Mister”  is formed by \\nexpansion, the token  “12” gets the orthographic \\nform “twelve” and “1997” is transformed to \\n“nineteen ninety seven”.  \\n\\uf0b7 Application of Pronunciation Rules:  After the text \\nanalysis has been completed, pronunciation rules \\ncan be applied. Letters cannot be transformed 1:1 \\ninto phonemes because correspondence is not \\nalways parallel. In ce rtain environments, a single \\nletter can correspond to either no phoneme (for \\nexample, “h” in “caught”) or several phoneme (“m” \\nin “Maximum”). In addition, several letters can ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='5ed7a16c-cfbc-4786-a950-b5f239ebfce3', embedding=None, metadata={'page_label': '3', 'file_name': 't2v_visual_impaired.pdf', 'file_path': '/kaggle/input/demo-db/t2v_visual_impaired.pdf', 'file_type': 'application/pdf', 'file_size': 1114479, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' International Journal of Applied Information Systems (IJAIS) – ISSN  : 2249 -0868   \\nFoundation of Computer Sci ence FCS, New York, USA  \\nVolume 7– No. 2, April 2014  – www.ijais.org  \\n \\n27 \\ncorrespond to a single phoneme (“ch” in “rich”). \\nThere are two strategies to dete rmine pronunciation:  \\n\\uf0b7 In dictionary -based solution with \\nmorphological components, as many \\nmorphemes (words) as possible are stored \\nin a dictionary. Full forms are generated \\nby means of inflection, derivation and \\ncomposition rules. Alternatively, a full \\nform  dictionary is used in which all \\npossible word forms are stored. \\nPronunciation rules determine the \\npronunciation of words not found in the \\ndictionary.  \\n\\uf0b7 In a rule based solution, pronunciation \\nrules are generated from the phonological \\nknowledge of dictionari es. Only words \\nwhose pronunciation is a complete \\nexception are included in the dictionary.  \\nThe two applications differ significantly in the size \\nof their dictionaries. The dictionary -based solution \\nis many times larger than the rules -based solution’s \\ndictionary of exception. However, dictionary -based \\nsolutions can be more exact than rule -based solution \\nif they have a large enough phonetic dictionary \\navailable.  \\n\\uf0b7 Prosody Generation: after the pronunciation has \\nbeen determined, the prosody is generated. The \\ndegree of naturalness of a TTS system is dependent \\non prosodic factors like intonation modelling \\n(phrasing and accentuation), amplitude modelling \\nand duration modelling (including the duration of \\nsound and the duration of pauses, which determines \\nthe length o f the syllable and the tempos of the \\nspeech) [16].  \\n \\nFigure 2: Operations of the natural Language processing \\nmodule of a TTS synthesizer . \\n The output of the NLP module is passed to the DSP module. \\nThis is where the actual synthesis of the speech signal \\nhappens. In concatenative synthesis the selection and linking \\nof speech segments take place. For individual sounds the best \\noption (where several appropriate options are available) are \\nselected from a database and concatenated.  \\nFigure 3: The DSP component  of a general concatenation -\\nbased synthesizer . [17]  \\n4. DESIGN & IMPLEMENTATION  \\nOur software is called the TextToSpeech Robot, a simple \\napplication with the text to speech functionality. The system \\nwas developed using Java programming language. Java is \\nused because it’s robust and independent platform.  \\nThe application is divided into two main modules - the main \\napplication module which includes the basic GUI components \\nwhich handles the basic operations of the application such as \\ninput of parameters for conve rsion either via file or direct \\nkeyboard input or the browser. This would make use of the \\nopen source API called SWT and DJNativeSwing.  \\nThe second module, the main conversion engine which \\nintegrated into the main module is for the acceptance of data \\nhence the conversion. This would implement the API called \\nfreeTTS.  \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='07e171b3-097a-43cb-bded-7d9daa4b1376', embedding=None, metadata={'page_label': '4', 'file_name': 't2v_visual_impaired.pdf', 'file_path': '/kaggle/input/demo-db/t2v_visual_impaired.pdf', 'file_type': 'application/pdf', 'file_size': 1114479, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' International Journal of Applied Information Systems (IJAIS) – ISSN  : 2249 -0868   \\nFoundation of Computer Sci ence FCS, New York, USA  \\nVolume 7– No. 2, April 2014  – www.ijais.org  \\n \\n28 \\nFigure 4: TTS Synthesis System Architecture . \\nTextToSpeech Robot (TTSR) converts text to speech either by \\ntyping the text into the text field provided or by coping from \\nan external document in t he local machine and then pasting it \\nin the text field provided in the application. It also provides a \\nfunctionality that allows the user browse the World Wide \\nWeb (www) on the application. TextToSpeech Robot is \\ncapable of reading any portion of the web pa ge the user \\nbrowses. This can be achieved by the user highlighting the \\nportion he wants to be read out loud by the TTSR and then \\nclicking on the “Play” button.  \\nTTSR contains an exceptional function that gives the user the \\nchoice of saving its already conve rted text to any part of the \\nlocal machine in an audio format; this allows the user to copy \\nthe audio format to any of his/her audio devices.  \\n \\nFigure 5: The Loading phase of the application . \\n  \\nFigure 6: Screenshot of the Text TO Speech Robot \\nInterface . \\n \\nThe default view for this application being created is the web \\nbrowser view. This is what shows after the TextToSpeech \\nRobot has loaded. The web browser displays that there is no \\ninternet connection on the local machine and so it displays \\n“The page cannot be displayed”. Any part of the web browser \\nin the application that is highlighted can be read out loud by \\nthe TTSR. The application allows the user to highlight any \\npart of the web page for conversion.  \\n \\n \\nFigure 7: A part of the web page in the application being \\nhighlighted waiting for conversion.  \\n \\nThe Standard Tool Bar  \\nThe standard tool bar contains the File, Web browser, Player \\nand Help  \\nThe File Menu gives the user a choice of selecting either to \\nopen a new browser or to open a new text field for text \\ndocument to be imported in. the Player Menu gives the user a \\nchoice to play the speech, stop the speech or pause the speech. \\nIt also has a functional but ton called “Record” this lets you \\nexport the audio speech to any part of your local machine.  \\nThe text field:  \\nThe text field is where all text is typed or loaded into. The text  \\nthat will be read by the en gine is contained in this field.  \\n \\nFREETS\\nMain \\nApplicationModuleText Processing \\nModuleText To Speech\\nTop Package::UserInput - Text/File mediaText\\nMessage2Text Input\\nAudio Output\\nAudioAudio Output to file(optional; on user selection)File\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='66c07804-4b57-4d21-8cbb-14869fbdb486', embedding=None, metadata={'page_label': '5', 'file_name': 't2v_visual_impaired.pdf', 'file_path': '/kaggle/input/demo-db/t2v_visual_impaired.pdf', 'file_type': 'application/pdf', 'file_size': 1114479, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' International Journal of Applied Information Systems (IJAIS) – ISSN  : 2249 -0868   \\nFoundation of Computer Sci ence FCS, New York, USA  \\nVolume 7– No. 2, April 2014  – www.ijais.org  \\n \\n29 \\nFigure 8: The TTSR Interface when a text document is \\nloaded into it.  \\n \\n \\n \\nFigure 9: Work in progress of the creation of the \\napplica tion in the NetBeans Environment.  \\n5. CONCLUSION  \\nText to speech synthesis is a rapidly growing aspect of \\ncomputer technology and is increasingly playing a more \\nimportant role in the way we interact with the system and \\ninterfaces across a variety of platforms. We have identified \\nthe various operations and processes involved in text to \\nspeech synthesis. We have also developed a very simp le and \\nattractive graphical user interface which allows the user to \\ntype in his/her text provided in the text field in the application. \\nOur system interfaces with a text to speech engine developed \\nfor American English. In future, we plan to make efforts to  \\ncreate engines for localized Nigerian language so as to make \\ntext to speech technology more accessible to a wider range of \\nNigerians. This already exists in some native languages e.g. \\nSwahili [18], Konkani [19], the Vietnamese synthesis system \\n[10] and th e Telugu language [20]. Another area of further \\nwork is the implementation of a text to speech system on \\nother platforms, such as telephony systems, ATM machines, \\nvideo games and any other platforms where text to speech \\ntechnology would be an added advanta ge and increase \\nfunctionality.  \\n6. REFERENCES  \\n[1] Lemmetty , S., 1999.  Review of Speech Syn1thesis \\nTechnology . Masters Dissertation, Helsinki Univers ity \\nOf Technology.  [2] Dutoit , T., 1993.  High quality text -to-speech synthesis of \\nthe French language . Doctoral dissertation, Faculte \\nPolytechnique de Mons.  \\n[3] Suendermann,  D., Höge,  H., and Black , A., 2010. \\nChallenges in Speech Synthesis . Chen,  F., Jokinen , K., \\n(eds.), Speech Technology , Springer Science + Business \\nMedia LLC.  \\n[4] Allen, J., Hunnicutt , M. S., Klatt  D., 1987. From Text to \\nSpeech: The MITalk system . Cambridge University \\nPress.  \\n[5] Rubin, P.,  Baer, T., and  Mermelstein, P., 1981. An \\narticulatory synthesizer for perceptual research . Journal \\nof the Acoustical Society of America 70: 321 –328. \\n[6] van Santen,  J.P.H., Sproat,  R. W.,  Olive, J.P., and \\nHirschberg, J., 1997. Progress in Speech Synthesis. \\nSpringer . \\n[7] van Santen, J.P.H. , 1994.  Assignment of segmental \\nduration in text -to-speech synthesi s. Computer Speech & \\nLanguage , Volume 8, Issue 2, Pages 95 –128 \\n[8] Wasala,  A., Weerasinghe R. , and Gamage, K., 2006, \\nSinhala Grapheme -to-Phoneme Conversion  and Rules \\nfor Schwaepenthesis.  Proceedings of the COLING/ACL \\n2006 Main Conference Poster Sessions, Sydney, \\nAustralia, pp. 890 -897. \\n[9] Lamel,  L.F., Gauvain,  J.L., Prouts,  B., Bouhier,  C., and  \\nBoesch, R., 1993.  Generation and Synthesis of Broadcast \\nMessages, Proceedings ESCA -NATO Workshop a nd \\nApplications  of Speech Technology . \\n[10] van Truc, T., Le Quang, P., van Thuyen,  V., Hieu, L.T., \\nTuan, N.M., and Hung  P.D., 2013 . Vietnamese Synthesis \\nSystem, Capstone Project Document, F PT \\nUNIVERSITY.  \\n[11] Black, A.W., 2002. Perfect synthesis for all of the people \\nall of the time. IEEE TTS Workshop.  \\n[12] Kominek , J., and Black, A.W., 2003 . CMU ARCTIC \\ndatabases for speech synthesis. CMU -LTI-03-177. \\nLanguage Technologies Institute, School of Computer \\nScience, Carnegie Mellon University.  \\n[13] Zhang , J., 2004 . Language Generation and Speech \\nSynthesis in D ialogues for Language Learning. Masters \\nDissertation, Massachusetts Institute of Technology.  \\n[14] Dutoit, T., Pagel, V., Pierret, N., Bataille,  F., van der \\nVrecken, O., 1996. The MBROLA Project: Towards a \\nset of high  quality speech synthesizers of use for non -\\ncommercial p urposes. ICSLP Proceedings . \\n[15] Text-to-speech (TTS) Overview. In Voice RSS Website. \\nRetrieved  February 21, 2014, from   \\nhttp://www.voicerss.org/tts/  \\n[16] Text-to-speech technology: In Linguatec Language \\nTechnology  Website. Retrieved  February 21, 2014, \\nfrom   \\nhttp://www.linguatec.net/products/tts/information/techno\\nlogy \\n[17] Dutoit , T.,  1997. High -Quality Text -to-Speech \\nSynthesis:An Overview. Journal Of Electrical And \\nElectronics Engineering Australia 17, 25 -36. \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='25c4e45c-ab3a-40c9-a636-e14856eef703', embedding=None, metadata={'page_label': '6', 'file_name': 't2v_visual_impaired.pdf', 'file_path': '/kaggle/input/demo-db/t2v_visual_impaired.pdf', 'file_type': 'application/pdf', 'file_size': 1114479, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' International Journal of Applied Information Systems (IJAIS) – ISSN  : 2249 -0868   \\nFoundation of Computer Sci ence FCS, New York, USA  \\nVolume 7– No. 2, April 2014  – www.ijais.org  \\n \\n30 \\n[18] Ngugi,  K., Okelo -Odongo,  W., and  Wagacha , P. W.,  \\n2005. Swahili Text -To-Speech System.  African Journal \\nof Science and Technolog y (AJST) Science and \\nEngineering Series Vol. 6, No. 1, pp. 80 – 89. \\n[19] Mohanan,  S., Salkar,  S., Naik, G., Dessai, N.B., and \\nNaik , S., 2012.  Text To Speech S ynthesizer for Konkani \\nLanguage . International Conference on Computing and Control Engineering ( ICCCE 2012), 12 & 13 April,  \\nISBN 978 -1-4675 -2248 -9. \\n[20] Swathi,  G., Mai, C. K., and Babu , B. R., 2013.  Speech \\nSynthesis System for Telugu Language.  International \\nJournal of Computer Applications (0975 – 8887), \\nVolume 81 – No5.  \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["documents=SimpleDirectoryReader(\"/kaggle/input/demo-db\").load_data()\n","documents"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:18:37.259365Z","iopub.status.busy":"2024-08-13T09:18:37.259053Z","iopub.status.idle":"2024-08-13T09:18:37.263873Z","shell.execute_reply":"2024-08-13T09:18:37.262941Z","shell.execute_reply.started":"2024-08-13T09:18:37.259341Z"},"trusted":true},"outputs":[],"source":["system_prompt=\"\"\"\n","You are a advanced research assistant. Your goal is to understand the paper given and provide the summary of it \n","mentioning its novelty along with papers referred in it  as\n","accurately as possible based on the instructions and context provided.\n","\"\"\"\n","query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:18:38.851686Z","iopub.status.busy":"2024-08-13T09:18:38.850965Z","iopub.status.idle":"2024-08-13T09:18:39.904296Z","shell.execute_reply":"2024-08-13T09:18:39.903168Z","shell.execute_reply.started":"2024-08-13T09:18:38.851645Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Aug 13 09:18:39 2024       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0             28W /  250W |       0MiB /  16384MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:18:39.906163Z","iopub.status.busy":"2024-08-13T09:18:39.905802Z","iopub.status.idle":"2024-08-13T09:18:52.586266Z","shell.execute_reply":"2024-08-13T09:18:52.584612Z","shell.execute_reply.started":"2024-08-13T09:18:39.906132Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.0.dev0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n"]}],"source":["!pip install --upgrade transformers\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:18:52.588092Z","iopub.status.busy":"2024-08-13T09:18:52.587755Z","iopub.status.idle":"2024-08-13T09:19:50.495105Z","shell.execute_reply":"2024-08-13T09:19:50.493924Z","shell.execute_reply.started":"2024-08-13T09:18:52.588061Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b24deeccbeeb44c79f807550d7e4d5c8","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40b0a5bcc70040fcb36d523c03ed7e29","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf255b2bf95d4b9797e130959024de6e","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0c5f67eb3a74c5da419941129761cce","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5af046b44cbb472bb2816a8490b30b27","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"798e25620eaf479ba311ce69fffaa9f1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"310ebc193ac54cbbb6b7ad8e00c80697","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20dcec163ab24f75952cc41ef0e5c341","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f170c3ace334eed8616b2de40022780","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f26da1be5ea4cf7a51704c78d9f1e6e","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"370086b39c984617946cec6b08f6fd46","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import transformers \n","\n","llm = HuggingFaceLLM(\n","    context_window=4096,\n","    max_new_tokens=256,\n","    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n","    system_prompt=system_prompt,\n","    query_wrapper_prompt=query_wrapper_prompt,\n","    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n","    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n","    device_map=0,\n","    model_kwargs={\"torch_dtype\": torch.float16 }\n",")\n","  "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:19:50.498421Z","iopub.status.busy":"2024-08-13T09:19:50.497905Z","iopub.status.idle":"2024-08-13T09:20:09.712073Z","shell.execute_reply":"2024-08-13T09:20:09.711139Z","shell.execute_reply.started":"2024-08-13T09:19:50.498395Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n","  warn_deprecated(\n","2024-08-13 09:19:52.761306: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-13 09:19:52.761424: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-13 09:19:52.890494: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2085bea3dd4948689b7251f32a3e7145","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57f4b97cb8a442ddb4c31ea23a74430a","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb16fc7a610b44638bec8b96726379b7","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2968bc106f2646c9a0ee511ba27823b7","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a86ce425b9dc4659a3078a91ba4ed26b","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb9f48529e1a4ef5a115261240727784","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba02e0788e814d1ea1ad05430ca18adc","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cea7c47609944ac3af51e49544df89da","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a5dbfe607b54786a7f00e34b7e46a14","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35e531482bbd4fb6a087edfd92cd2ed4","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37d6b94f8bd3421a9227a124bb9450bf","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["embed_model=LangchainEmbedding(\n","    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:20:09.714857Z","iopub.status.busy":"2024-08-13T09:20:09.713411Z","iopub.status.idle":"2024-08-13T09:20:11.104582Z","shell.execute_reply":"2024-08-13T09:20:11.103555Z","shell.execute_reply.started":"2024-08-13T09:20:09.714822Z"},"trusted":true},"outputs":[],"source":["service_context=ServiceContext.from_defaults(\n","    chunk_size=1024,\n","    llm=llm,\n","    embed_model=embed_model\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:20:11.106138Z","iopub.status.busy":"2024-08-13T09:20:11.105821Z","iopub.status.idle":"2024-08-13T09:20:11.113088Z","shell.execute_reply":"2024-08-13T09:20:11.111891Z","shell.execute_reply.started":"2024-08-13T09:20:11.106095Z"},"trusted":true},"outputs":[{"data":{"text/plain":["ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7c3bf9cef640>, num_workers=None), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7c3bf9cef640>, id_func=<function default_id_func at 0x7c3f967c0310>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.core.service_context_elements.llama_logger.LlamaLogger object at 0x7c3f831c6f20>, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7c3bf9cef640>)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["service_context"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:20:11.114820Z","iopub.status.busy":"2024-08-13T09:20:11.114560Z","iopub.status.idle":"2024-08-13T09:20:12.719108Z","shell.execute_reply":"2024-08-13T09:20:12.718167Z","shell.execute_reply.started":"2024-08-13T09:20:11.114795Z"},"trusted":true},"outputs":[],"source":["index=VectorStoreIndex.from_documents(documents,service_context=service_context)\n","     "]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:20:12.721128Z","iopub.status.busy":"2024-08-13T09:20:12.720454Z","iopub.status.idle":"2024-08-13T09:20:12.727043Z","shell.execute_reply":"2024-08-13T09:20:12.726165Z","shell.execute_reply.started":"2024-08-13T09:20:12.721077Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7c3bf4270cd0>"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["index"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:20:12.728317Z","iopub.status.busy":"2024-08-13T09:20:12.728018Z","iopub.status.idle":"2024-08-13T09:20:12.738036Z","shell.execute_reply":"2024-08-13T09:20:12.737267Z","shell.execute_reply.started":"2024-08-13T09:20:12.728286Z"},"trusted":true},"outputs":[],"source":["query_engine=index.as_query_engine(do_sample=True)"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:30:28.945200Z","iopub.status.busy":"2024-08-13T09:30:28.944255Z","iopub.status.idle":"2024-08-13T09:30:50.119469Z","shell.execute_reply":"2024-08-13T09:30:50.118630Z","shell.execute_reply.started":"2024-08-13T09:30:28.945163Z"},"trusted":true},"outputs":[],"source":["summary =query_engine.query(\"what is the summary of the paper t2speech?\")"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:30:50.121367Z","iopub.status.busy":"2024-08-13T09:30:50.121040Z","iopub.status.idle":"2024-08-13T09:30:50.127746Z","shell.execute_reply":"2024-08-13T09:30:50.126915Z","shell.execute_reply.started":"2024-08-13T09:30:50.121339Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Response(response='\\nThe paper \"T2Speech\" presents a high-level diagram of a text-to-speech (TTS) system, which takes text as input and generates audio output. The system consists of several components, including a preprocessor, encoder, decoder, and vocoder. The preprocessor tokenizes the input text into words, breaks them into phonemes based on their pronunciation, and represents the phoneme duration and pitch. The encoder inputs the linguistic features (phonemes) and outputs an n-dimensional embedding. The decoder converts the embedded information to an acoustic feature (mel-spectrogram), which is then converted to audio output by the vocoder. The authors propose a two-stage approach, where the decoder first predicts the mel-spectrogram from the latent processed features and then generates audio using the mel-spectrogram. The paper also discusses the importance of prosody in TTS and how it can be achieved through the use of energy, pitch, and duration predictors.\\n\\nAs for the novelty of the paper, it presents a new approach to TTS by proposing a two-stage decoder that first predicts the mel-spectro', source_nodes=[NodeWithScore(node=TextNode(id_='65fd2899-3123-4537-b19c-36ea21110650', embedding=None, metadata={'page_label': '4', 'file_name': 'a survey of ai t2i and t2v.pdf', 'file_path': '/kaggle/input/demo-db/a survey of ai t2i and t2v.pdf', 'file_type': 'application/pdf', 'file_size': 403891, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='98c897e8-c02b-460b-9591-d805c271627e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'a survey of ai t2i and t2v.pdf', 'file_path': '/kaggle/input/demo-db/a survey of ai t2i and t2v.pdf', 'file_type': 'application/pdf', 'file_size': 403891, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, hash='b925fdaa8d1c622d07d8524755c399800efd2fac4b4b72c01266d09dcb6873c7'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='bf4ef1dc-53ac-4a48-b7cf-0b78b1683aea', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '4', 'file_name': 'a survey of ai t2i and t2v.pdf', 'file_path': '/kaggle/input/demo-db/a survey of ai t2i and t2v.pdf', 'file_type': 'application/pdf', 'file_size': 403891, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, hash='ce7cfc908aeb3325136a2aa6bb16e7f1c14396c7b96afa800f25cc87ddbcde63')}, text='2022. [15] W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang, \"CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,\" arXiv:2205.15868 [cs.CV], May 2022. [16] C. Wu, L. Huang, Q. Zhang, B. Li, L. Ji, F. Yang, G. Sapiro, and N. Duan, \"GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions,\" arXiv:2104.14806, Apr. 2021. [17] C. Wu, J. Liang, L. Ji, F. Yang, Y. Fang, D. Jiang, and N. Duan, \"NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,\" arXiv:2111.12417 [cs.CV], Nov. 2021.  [18] B. Bordia and S. R. Bowman, \"Identifying and Reducing Gender Bias in Word-Level Language Models,\" arXiv:1904.03035 [cs.CL], 2019. [19] A. Birhane, V. U. Prabhu, and E. Kahembwe, \"Multimodal Datasets: Misogyny, Pornography, and Malignant Stereotypes,\" arXiv:2110.01963, 2021. [20] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, \"On the dangers of stochastic parrots: Can language models be too big?\" in Proc. FAccT, 2021.', mimetype='text/plain', start_char_idx=5454, end_char_idx=6428, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.27909675791873656), NodeWithScore(node=TextNode(id_='cf749f34-81aa-4e5a-baac-5b73221dcffe', embedding=None, metadata={'page_label': '2', 'file_name': 't2speech.pdf', 'file_path': '/kaggle/input/demo-db/t2speech.pdf', 'file_type': 'application/pdf', 'file_size': 281862, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='224a835d-0627-41c2-987d-ef41d263153c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '2', 'file_name': 't2speech.pdf', 'file_path': '/kaggle/input/demo-db/t2speech.pdf', 'file_type': 'application/pdf', 'file_size': 281862, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, hash='b2beb87504bab6074d843d173691c189362fd63859ab167f6d3a1c1f7234cf6a')}, text='This is a high-level  diagram  of different  components  used in \\nthe TTS system.  The input  to our model  is text, which  passes  \\nthrough  several  blocks  and eventually  is converted  to audio.   \\n \\nPreproces sor \\n \\n● Tokenize : Tokenize  a sentence  into words  \\n● Phonemes/Pronunciation:  It breaks  input  text into \\nphonemes,  based  on their pronunciation.  For \\nexample,  “Hello,  Have  a good  day”  converts  to HH \\nAH0  L OW1,  HH AE1 V AH0  G UH1  D D EY1.  \\n● Phoneme  duration:  Represents  the total time taken  \\nby each phoneme  in the audio.  \\n● Pitch : Key feature  to convey  emotions,  it greatly  \\naffects  the speech  prosody.  \\n● Energy : Indicates  frame -level  magnitude  of mel-\\nspectrograms  and directly  affects  the volume  and \\nprosody  of speech.  \\nThe Linguistic  feature  only contains  phonemes.  Energy,  \\npitch,  and duration  are actually  used to train the energy  \\npredictor,  the pitch  predictor,  and the duration  predictor  \\nrespectively  which  are used by the model  to get a more  natural  \\noutput.  \\n \\n \\nEncoder  \\n \\n \\n \\nThe encoder  inputs  Linguistic  features  (Phonemes)  and \\noutputs  an n-dimensional  embedding.  This embedding  \\nbetween  the encoder  and decoder  is known  as the latent  \\nfeature.  Latent  features  are crucial  because,  other  features  like \\nspeaker  embedding  are concat enated  with these  and passed  to \\nthe decoder.  Furthermore,  the latent  features  are also used for \\nthe prediction  of energy,  pitch,  and duration  which  in turn \\nplay a crucial  role in controlling  the naturalness  of the audio.  \\n \\nDecoder   \\nThe decoder  is used to convert  information  embedded  in the \\nLatent  processed  feature  to the Acoustic  feature  i.e. Mel-\\nspectrogram.  \\n \\nVocoder  \\n \\nIt converts  the Acoustic  feature  (Mel -spectrogram)  to \\nwaveform  output  (audio).  It can be done  using  a mathematical  \\nmodel  like Griffin  Lim or we can also train a neural  network  \\nto learn  the mapping  from  mel-spectrogram  to waveforms.  In \\nreality,  learning -based  methods  usually  outperform  the \\nGriffin  Lim method.  \\n \\nSo instead  of directly  predicting  waveform  using  the decoder,  \\nwe split this complex  and sophisticated  task into two stages,  \\nfirst predicting  mel-spectrogram  from  Latent  processed  \\nfeatures  and then generating  audio  using  mel-spectrogram.  \\n  \\nLITERATURE  REVIEW  \\nDesigning  an effective  text-to-speech  synthesis  system  is \\nquite  difficult.  Building  a whole  TTS system  requires  \\ncompleting  several  steps,  including  normalizing  text, \\nconverting  text to phonemes,  identifying  prosodic  emotional  \\ncontent,  and generating  speec h. \\nSpeech  synthesis  for different  languages  has already  been  the \\nsubject  of many  research  proposals.  Before  electronic  signal  \\nprocessing  was invented,  some  early  scientists  tried to make  \\nmachines  that could  mimic  human  speech.  \\n \\nA Unit Selection  approach  for the text-to-speech  synthesis  \\nusing  Syllabic  was presented  in [1]. In this paper,  They  select  \\nsyllables  as their unit- hence  this was the first syllable  based  \\nText to Speech  conversion  system  for Bangla  language.  In \\nthis System,  It is necessary  to conduct  a substantial  amount  of \\ntesting  with an even  larger  text corpus  than the one they \\nutilized  as an experimental  text corpus.  \\n \\nThe research  by F. Alam  and colleagues  resulted  in the \\ndevelopment  of a speech  synthesizer  for the Bangla  \\nlanguage[2,3].  The diphon e concatenation  method  was used', mimetype='text/plain', start_char_idx=3, end_char_idx=3564, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.2699776442130846)], metadata={'65fd2899-3123-4537-b19c-36ea21110650': {'page_label': '4', 'file_name': 'a survey of ai t2i and t2v.pdf', 'file_path': '/kaggle/input/demo-db/a survey of ai t2i and t2v.pdf', 'file_type': 'application/pdf', 'file_size': 403891, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, 'cf749f34-81aa-4e5a-baac-5b73221dcffe': {'page_label': '2', 'file_name': 't2speech.pdf', 'file_path': '/kaggle/input/demo-db/t2speech.pdf', 'file_type': 'application/pdf', 'file_size': 281862, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}})"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["summary"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:30:50.129315Z","iopub.status.busy":"2024-08-13T09:30:50.128819Z","iopub.status.idle":"2024-08-13T09:30:50.139830Z","shell.execute_reply":"2024-08-13T09:30:50.138979Z","shell.execute_reply.started":"2024-08-13T09:30:50.129290Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\nThe paper \"T2Speech\" presents a high-level diagram of a text-to-speech (TTS) system, which takes text as input and generates audio output. The system consists of several components, including a preprocessor, encoder, decoder, and vocoder. The preprocessor tokenizes the input text into words, breaks them into phonemes based on their pronunciation, and represents the phoneme duration and pitch. The encoder inputs the linguistic features (phonemes) and outputs an n-dimensional embedding. The decoder converts the embedded information to an acoustic feature (mel-spectrogram), which is then converted to audio output by the vocoder. The authors propose a two-stage approach, where the decoder first predicts the mel-spectrogram from the latent processed features and then generates audio using the mel-spectrogram. The paper also discusses the importance of prosody in TTS and how it can be achieved through the use of energy, pitch, and duration predictors.\\n\\nAs for the novelty of the paper, it presents a new approach to TTS by proposing a two-stage decoder that first predicts the mel-spectro'"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["summary.response"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:32:26.768782Z","iopub.status.busy":"2024-08-13T09:32:26.768412Z","iopub.status.idle":"2024-08-13T09:32:48.406310Z","shell.execute_reply":"2024-08-13T09:32:48.405535Z","shell.execute_reply.started":"2024-08-13T09:32:26.768754Z"},"trusted":true},"outputs":[],"source":["novel = query_engine.query(\"what is the so unique about this paper  t2speech? tell me in bullet points \")"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:32:58.393978Z","iopub.status.busy":"2024-08-13T09:32:58.393272Z","iopub.status.idle":"2024-08-13T09:32:58.400128Z","shell.execute_reply":"2024-08-13T09:32:58.399160Z","shell.execute_reply.started":"2024-08-13T09:32:58.393951Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Response(response='\\n\\nThe paper \"T2Speech: Text-to-Speech with Neural Voices\" (page 1) presents several novel aspects in the field of text-to-speech (TTS) systems:\\n\\n1. **Neural Voices:** The paper introduces the concept of \"Neural Voices,\" which are generated using deep neural networks. These voices sound more natural than standard voices and produce human-like speech patterns, such as stress and loudness of individual words.\\n2. **Custom Neural Voice:** The paper proposes a custom neural voice option that uses the user\\'s own audio data to create a one-of-a-kind, customized synthetic voice. This level of voice personalization is not commonly found in TTS systems.\\n3. **Deepest Level of Voice Personalization:** Custom Neural Voice offers the deepest level of voice personalization, with realistic speech that can be used to represent brands, personify machines, and allow users to interact with applications conversationally.\\n4. **Phoneme, Prosody, and Mel-spectrogram:** The paper explains the importance of phoneme, prosody', source_nodes=[NodeWithScore(node=TextNode(id_='bf4ef1dc-53ac-4a48-b7cf-0b78b1683aea', embedding=None, metadata={'page_label': '4', 'file_name': 'a survey of ai t2i and t2v.pdf', 'file_path': '/kaggle/input/demo-db/a survey of ai t2i and t2v.pdf', 'file_type': 'application/pdf', 'file_size': 403891, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='98c897e8-c02b-460b-9591-d805c271627e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'a survey of ai t2i and t2v.pdf', 'file_path': '/kaggle/input/demo-db/a survey of ai t2i and t2v.pdf', 'file_type': 'application/pdf', 'file_size': 403891, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, hash='b925fdaa8d1c622d07d8524755c399800efd2fac4b4b72c01266d09dcb6873c7'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='eb173075-1733-4b1f-bdf9-e05d1247d427', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '4', 'file_name': 'a survey of ai t2i and t2v.pdf', 'file_path': '/kaggle/input/demo-db/a survey of ai t2i and t2v.pdf', 'file_type': 'application/pdf', 'file_size': 403891, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, hash='0ee8a697869d191d6765ca397c594321a715d716e84a03d54194cf4fb9a57e2b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='65fd2899-3123-4537-b19c-36ea21110650', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='802f361851186c612df8506658a89ecf80897a9f1da42520466bd1c28dc6dfcf')}, text='3, pp. 119-132, 2022. doi: 10.1016/j.ijin.2022.08.005.  [6] S. Aktay, \"The usability of Images Generated by Artificial Intelligence (AI) in Education,\" International Technology and Education Journal, vol. 6, no. 2, pp. 51-62, 2022. [Online]. Available: https://dergipark.org.tr/en/pub/itej/issue/75198/1233537.  [7] E. Cetinic and J. She, \"Understanding and Creating Art with AI: Review and Outlook,\" ACM Trans. Multimedia Comput. Commun. Appl., vol. 18, no. 2, Article 66, May 2022, pp. 1-22, doi: 10.1145/3475799.  [8] M. Ding, W. Zheng, W. Hong, and J. Tang, \"CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers,\" arXiv, 2022. [Online]. Available: https://arxiv.org/abs/2204.14217. [Accessed: March 18, 2023]. [9] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou, Z. Shao, H. Yang, and J. Tang, \"CogView: Mastering Text-to-Image Generation via Transformers,\" arXiv:2105.13290 [cs.CV], 2021.  [10] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \"Hierarchical Text-Conditional Image Generation with CLIP Latents,\" in arXiv preprint arXiv:2202.10775, 202  [11] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. Seyed Ghasemipour, B. Karagol Ayan, S. S. Mahdavi, R. G. Lopes, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi, \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,\" arXiv:2205.11487 [cs.CV], May 2022. [12] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, D. Parikh, S. Gupta, and Y. Taigman, \"Make-A-Video: Text-to-Video Generation without Text-Video Data,\" arXiv:2209.14792 [cs.CV], Sep. 2022. [13] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, and T. Salimans, \"Imagen Video: High Definition Video Generation with Diffusion Models,\" arXiv preprint arXiv:2210.02303, Oct. 2022. [Online]. Available: https://arxiv.org/abs/2210.02303. [14] R. Villegas, M. Babaeizadeh, P.-J. Kindermans, H. Moraldo, H. Zhang, M. T. Saffar, S. Castro, J. Kunze, and D. Erhan, \"Phenaki: Variable Length Video Generation from Open Domain Textual Description,\" arXiv:2210.02399 [cs.CV], Oct. 2022. [15] W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang, \"CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,\" arXiv:2205.15868 [cs.CV], May 2022. [16] C. Wu, L. Huang, Q. Zhang, B. Li, L. Ji, F. Yang, G. Sapiro, and N. Duan, \"GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions,\" arXiv:2104.14806, Apr. 2021.', mimetype='text/plain', start_char_idx=3259, end_char_idx=5807, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.25016431869320227), NodeWithScore(node=TextNode(id_='1174840e-2125-48d5-a92d-d7836578eaef', embedding=None, metadata={'page_label': '1', 'file_name': 't2speech.pdf', 'file_path': '/kaggle/input/demo-db/t2speech.pdf', 'file_type': 'application/pdf', 'file_size': 281862, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d05e65bc-9722-488b-a13c-957c4038895a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 't2speech.pdf', 'file_path': '/kaggle/input/demo-db/t2speech.pdf', 'file_type': 'application/pdf', 'file_size': 281862, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, hash='0c2ff94597a3f62d4069127d8b0c94ff3c76027aaa2a5eb50148def71891362a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='18bebfe1-980b-4781-85c3-f9d4aa4c04d0', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 't2speech.pdf', 'file_path': '/kaggle/input/demo-db/t2speech.pdf', 'file_type': 'application/pdf', 'file_size': 281862, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, hash='59f58447aa680b0400f3f90c21fe10e9108a8ce06cde44f8883ebc5a88351e30')}, text='Powered  by deep  neural  networks,  neural  voices  \\nsound  more  natural  than standard  voices  by producing  \\nhuman -like speech  patterns,  such as stress  and loudness  of \\nindividual  words.  Because  of this human -like speech,  you \\nend up with a more  precise  articulation  of words,  along  with \\na significant  reduction  in listening  fatigue  when  users  \\ninteract  with AI systems.  \\nCustom  Neural  Voice:  Custom  neural  voice  uses your own \\naudio  data to create  a one-of-a-kind customized  synthetic  \\nvoice.  Custom  neural  voice  has the deepest  level  of voice \\npersonalization,  with realistic  speech  that can be used to \\nrepresent  brands,  personify  machines,  and allow  users  to \\ninteract  with applications  conversationally.   \\nB. Some  Terminology   \\nPhoneme:  A phoneme  is the smallest  unit of sound  that \\nmakes  a word’s  pronunciation  and meaning  different  from  \\nanother  word.  \\nProsody:  The patterns  of rhythm  and sound  used in poetry.   \\nMel-spectrogram:  It is derived  by applying  a non-linear  \\ntransformation  to the frequency  axis of short -time Fourier  \\ntransform  (STFT)  of audio,  to reduce  the dimensionality.  It \\nemphasizes  details  in low frequencies  which  are very \\nimportant  to distinguish  speech  and de-emphasizes  details  in \\nhigh frequencies  which  usually  are noise.  \\nText -To-Speech  (TTS)  Structure  \\n \\n \\nFig. 1 : Text-to-speech  structure', mimetype='text/plain', start_char_idx=2967, end_char_idx=4400, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.2421890654734739)], metadata={'bf4ef1dc-53ac-4a48-b7cf-0b78b1683aea': {'page_label': '4', 'file_name': 'a survey of ai t2i and t2v.pdf', 'file_path': '/kaggle/input/demo-db/a survey of ai t2i and t2v.pdf', 'file_type': 'application/pdf', 'file_size': 403891, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, '1174840e-2125-48d5-a92d-d7836578eaef': {'page_label': '1', 'file_name': 't2speech.pdf', 'file_path': '/kaggle/input/demo-db/t2speech.pdf', 'file_type': 'application/pdf', 'file_size': 281862, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}})"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["novel"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:33:11.755528Z","iopub.status.busy":"2024-08-13T09:33:11.754865Z","iopub.status.idle":"2024-08-13T09:33:11.761333Z","shell.execute_reply":"2024-08-13T09:33:11.760303Z","shell.execute_reply.started":"2024-08-13T09:33:11.755485Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\n\\nThe paper \"T2Speech: Text-to-Speech with Neural Voices\" (page 1) presents several novel aspects in the field of text-to-speech (TTS) systems:\\n\\n1. **Neural Voices:** The paper introduces the concept of \"Neural Voices,\" which are generated using deep neural networks. These voices sound more natural than standard voices and produce human-like speech patterns, such as stress and loudness of individual words.\\n2. **Custom Neural Voice:** The paper proposes a custom neural voice option that uses the user\\'s own audio data to create a one-of-a-kind, customized synthetic voice. This level of voice personalization is not commonly found in TTS systems.\\n3. **Deepest Level of Voice Personalization:** Custom Neural Voice offers the deepest level of voice personalization, with realistic speech that can be used to represent brands, personify machines, and allow users to interact with applications conversationally.\\n4. **Phoneme, Prosody, and Mel-spectrogram:** The paper explains the importance of phoneme, prosody'"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["novel.response"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:34:38.106348Z","iopub.status.busy":"2024-08-13T09:34:38.105876Z","iopub.status.idle":"2024-08-13T09:34:57.279034Z","shell.execute_reply":"2024-08-13T09:34:57.278041Z","shell.execute_reply.started":"2024-08-13T09:34:38.106317Z"},"trusted":true},"outputs":[],"source":["validity = query_engine.query(\"Does this paper have any value in current trends or scenarios ? if so list out and also explain why with matching trends or tech\")"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:35:06.093210Z","iopub.status.busy":"2024-08-13T09:35:06.092829Z","iopub.status.idle":"2024-08-13T09:35:06.099394Z","shell.execute_reply":"2024-08-13T09:35:06.098482Z","shell.execute_reply.started":"2024-08-13T09:35:06.093180Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Response(response='\\nBased on the provided context, the paper \"Text to Speech Conversion\" published in Indian Journal of Science and Technology in October 2016, has some value in current trends and scenarios. Here are some reasons why:\\n\\n1. Deep Learning-based OCR: The paper discusses the use of deep learning-based Optical Character Recognition (OCR) techniques for text to speech conversion. This is a current trend in the field of Natural Language Processing (NLP) as deep learning-based models have shown better performance in OCR tasks compared to traditional machine learning-based approaches.\\n2. Conversational AI: The paper proposes a text-to-speech conversion system that can convert text to speech in real-time, which is a key component of conversational AI. With the growing interest in conversational AI, the paper\\'s approach could be useful in developing chatbots and virtual assistants that can communicate with users in a more natural way.\\n3. Voice Assistants: The paper\\'s focus on text-to-speech conversion for visually impaired individuals could be relevant in the current scenario where voice assistants like Amazon Alexa', source_nodes=[NodeWithScore(node=TextNode(id_='cf3ed78c-6cc3-4103-a9e4-ddc6d776f7fe', embedding=None, metadata={'page_label': '4', 'file_name': 'design and implementation.pdf', 'file_path': '/kaggle/input/demo-db/design and implementation.pdf', 'file_type': 'application/pdf', 'file_size': 257923, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='839f600d-d488-4164-b11f-e24025a062e4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'design and implementation.pdf', 'file_path': '/kaggle/input/demo-db/design and implementation.pdf', 'file_type': 'application/pdf', 'file_size': 257923, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, hash='ea2a446de65e81b412c08275093c925cd6055f1e8cbb22222704032576aaae99')}, text='Indian Journal of Science and Technology3\\nVol 9 (38) | October 2016 | www.indjst.org S. Venkateswarlu, D. B. K. Kamesh, J. K. R. Sastry and Radhika Ranithis method, we can make editing process of books or web \\npages easier. \\n5. References\\n1. A rchana A, Shinde D. Text pre-processing and text seg-\\nmentation for OCR. International Journal of Computer \\nScience Engineering and Technology. 2012:810–12. \\n2.\\n M\\nithe R, Indalkar S, Divekar N. Optical character recog-\\nnition. International Journal of Recent Technology and Engineering. 2013 Mar; 2(1). \\n3.\\n S\\nmith R. An overview of the Tesseract OCR engine, USA: \\nGoogle Inc; 2007. \\n4.\\n S\\nhah H, Shah A. Optical character recognition of Gujarati \\nnumerical. International Conference on Signals, Systems and Automation. 2009; 49–53. \\n5.\\n M\\nonk S. Raspberry pi cook. \\n6.\\n T\\next localization and extraction in images using mathemat-\\nical\\n m\\norphology and OCR Techniques; 2013.\\n7.\\n V\\nanitha E, Kasarla PK, Kuamarswamy E. Implementation \\nof text- to-speech for real time embedded system using Raspberry Pi processor. International Journal and Magazine of Engineering Technology Management and Research. 2015 Jul:1995.\\n8.\\n  K\\numar GS, Krishna MNVLM. Low cost speech recognition \\nsystem running on Raspberry Pi to support Automation applications. International Journal of Engineering Trends and Technology. 2015; 21(5).\\n9.\\n B\\nhargava A, Nath KV , Sachdeva P , Samel M. Reading assis-\\ntant for visually Impaired. International Journal of current Engineering and Technology. 2015 Apr; 5(2). \\n10.\\n G\\nomes LCT, Nagle EJ, Chiquito JG. Text-to-speech conver -\\nsion system for Brazilian Portuguese using a formant-based synthesis technique. LPS-DECOM-FEEC-Unicamp.\\n11.\\n S\\nim Liew Fong, Abdelrahman Osman Elfaki, Md Gapar \\nbin Md Johar & Kevin Loo Tow Aik, Mobile Language Translator, 5th Malaysian Conference in Software Engineering (Misses); 2011.\\n12.\\n K\\namesh DBK, Nazma SK, Sastry JKR, Venkateswarlu S. \\nCamera based text to speech conversion, obstacle and cur -\\nrency detection for blind persons. Indian Journal of Science and Technology. 2016 Aug; 9(30).\\nView publication stats', mimetype='text/plain', start_char_idx=0, end_char_idx=2114, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.20307348447218174), NodeWithScore(node=TextNode(id_='f34f37eb-4548-4d9a-bdc0-d56830f3db7f', embedding=None, metadata={'page_label': '1', 'file_name': 'design and implementation.pdf', 'file_path': '/kaggle/input/demo-db/design and implementation.pdf', 'file_type': 'application/pdf', 'file_size': 257923, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='de971b99-a448-4e7a-a553-a274324cc810', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'design and implementation.pdf', 'file_path': '/kaggle/input/demo-db/design and implementation.pdf', 'file_type': 'application/pdf', 'file_size': 257923, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, hash='5c9672dc1d4b399f717c042390d7dcb0a82c3d03e29c1bfa640eef63c4f0a27d')}, text='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/313389686\\nText to Speech Conversion\\nArticle \\xa0\\xa0 in\\xa0\\xa0Indian Journal of Scienc e and T echnolog y · Oct ober 2016\\nDOI: 10.17485/ ijst/2016/v9i38/102967\\nCITATIONS\\n36READS\\n48,638\\n4 author s:\\nS. Venk atesw arlu\\nK L Univ ersity\\n28 PUBLICA TIONS \\xa0\\xa0\\xa094 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nDuvvuri B K Kamesh Duvvuri\\nMalla Malla R eddy Engineering c olle ge for Women\\n34 PUBLICA TIONS \\xa0\\xa0\\xa0177 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nSastr y Jammalamadak a\\nK L Univ ersity\\n95 PUBLICA TIONS \\xa0\\xa0\\xa0493 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nChint ala R adhik a Rani\\nK L Univ ersity\\n22 PUBLICA TIONS \\xa0\\xa0\\xa077 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Duvvuri B K Kamesh Duvvuri  on 23 Sept ember 2017.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', mimetype='text/plain', start_char_idx=0, end_char_idx=863, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.20028132179219918)], metadata={'cf3ed78c-6cc3-4103-a9e4-ddc6d776f7fe': {'page_label': '4', 'file_name': 'design and implementation.pdf', 'file_path': '/kaggle/input/demo-db/design and implementation.pdf', 'file_type': 'application/pdf', 'file_size': 257923, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}, 'f34f37eb-4548-4d9a-bdc0-d56830f3db7f': {'page_label': '1', 'file_name': 'design and implementation.pdf', 'file_path': '/kaggle/input/demo-db/design and implementation.pdf', 'file_type': 'application/pdf', 'file_size': 257923, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}})"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["validity"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T09:35:13.899395Z","iopub.status.busy":"2024-08-13T09:35:13.898608Z","iopub.status.idle":"2024-08-13T09:35:13.905215Z","shell.execute_reply":"2024-08-13T09:35:13.904362Z","shell.execute_reply.started":"2024-08-13T09:35:13.899365Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\nBased on the provided context, the paper \"Text to Speech Conversion\" published in Indian Journal of Science and Technology in October 2016, has some value in current trends and scenarios. Here are some reasons why:\\n\\n1. Deep Learning-based OCR: The paper discusses the use of deep learning-based Optical Character Recognition (OCR) techniques for text to speech conversion. This is a current trend in the field of Natural Language Processing (NLP) as deep learning-based models have shown better performance in OCR tasks compared to traditional machine learning-based approaches.\\n2. Conversational AI: The paper proposes a text-to-speech conversion system that can convert text to speech in real-time, which is a key component of conversational AI. With the growing interest in conversational AI, the paper\\'s approach could be useful in developing chatbots and virtual assistants that can communicate with users in a more natural way.\\n3. Voice Assistants: The paper\\'s focus on text-to-speech conversion for visually impaired individuals could be relevant in the current scenario where voice assistants like Amazon Alexa'"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["validity.response"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5188612,"sourceId":9163404,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
